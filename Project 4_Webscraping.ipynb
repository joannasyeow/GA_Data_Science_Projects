{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.set(rc={\"figure.figsize\": (15, 8)})\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:900px;background:#F9EECF;border:1px solid black;text-align:left;padding:8px;\">\n",
    "\n",
    "\n",
    "\n",
    "<p>\n",
    "<span style=\"font-size:14pt\"><b>Scraping from indeed.com</span></b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scraping from indeed.com\n",
    "\n",
    "country_url = {\"SG\":\"https://www.indeed.com.sg/jobs\",\n",
    "       \"US\":\"https://www.indeed.com/jobs\",\n",
    "       \"MY\":\"https://www.indeed.com.my/jobs\",\n",
    "       \"HK\":\"https://www.indeed.hk/jobs\",\n",
    "       \"ID\":\"https://id.indeed.com/lowongan-kerja\"\n",
    "       \n",
    "       }\n",
    "\n",
    "countries = {\"SG\":\"Singapore\",\"US\":\"United States\",\"MY\":\"Malaysia\",\"HK\":\"Hong Kong\",\"ID\":'Indonesia'}\n",
    "\n",
    "target_cities= {'US':\n",
    "                      ['New York', 'Chicago', 'San Francisco', 'Austin', 'Seattle',\n",
    "                  'Los Angeles', 'Philadelphia', 'Atlanta', 'Dallas',\n",
    "                  'Pittsburgh', 'Portland', 'Phoenix', 'Denver', 'Houston','Miami'],\n",
    "                'SG':[\"Singapore\"],\n",
    "                'MY':['Kuala Lumpur','Johor Bahru','Shah Alam'],\n",
    "                'HK':['Hong Kong'],\n",
    "                'ID':['Jakarta','Batam','Surabaya']\n",
    "                }\n",
    "\n",
    "job_titles = ['data scientist', 'data analyst','chief data officer','chief information officer',\\\n",
    "              'data engineer','business intelligence','artifical intelligence','machine learning'\\\n",
    "             'data consultant','marketing analyst','marketing intelligence','deep learning',\\\n",
    "             'chatbot','system analyst','data crawling','data entry','data administrator',\\\n",
    "             'nlp','data analytics','data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_title(chunk): \n",
    "    \n",
    "    job_titles = []\n",
    "    \n",
    "    try:\n",
    "        title = chunk.find('h2',{'class':'jobtitle'}).get_text()\n",
    "        job_titles.append(title)\n",
    "    except:\n",
    "        job_titles.append(np.nan)\n",
    "        \n",
    "    return job_titles\n",
    "\n",
    "def scrape_company(chunk):\n",
    "    \n",
    "    companies = []\n",
    "    \n",
    "    try:\n",
    "        company = chunk.find('span',{'class':'company'}).get_text()\n",
    "        companies.append(company)\n",
    "    except:\n",
    "        companies.append(np.nan)\n",
    "        \n",
    "    return companies\n",
    "\n",
    "def scrape_location(chunk):\n",
    "    \n",
    "    locations = []\n",
    "    \n",
    "    try:\n",
    "        location = chunk.find('span',{'class':'location'}).get_text()\n",
    "        locations.append(location)\n",
    "    except:\n",
    "        locations.append(np.nan)\n",
    "        \n",
    "    return locations\n",
    "\n",
    "def scrape_review(chunk):\n",
    "    \n",
    "    reviews = []\n",
    "    \n",
    "    try:\n",
    "        review = chunk.find('span',{'class':'slNoUnderline'}).get_text()\n",
    "        review_ = int(review.replace(' reviews','').replace(',',''))\n",
    "        reviews.append(review_)\n",
    "    except:\n",
    "        reviews.append(np.nan)\n",
    "        \n",
    "    return reviews\n",
    "\n",
    "def scrape_rating(chunk):\n",
    "    \n",
    "    ratings = []\n",
    "    \n",
    "    try:\n",
    "        rating = chunk.find('span',{'class':'rating'})\n",
    "        ratings.append(float(rating.get('style').replace('width:','').replace('px','')))\n",
    "        \n",
    "    except:\n",
    "        ratings.append(np.nan)\n",
    "        \n",
    "    return ratings\n",
    "\n",
    "def scrape_salary(chunk):\n",
    "    \n",
    "    salaries = []\n",
    "    \n",
    "    #get_extra = soup.findAll(\"div\", {\"data-tn-component\":\"organicJob\"})\n",
    "\n",
    "    #for chunk in get_extra:\n",
    "\n",
    "    try:\n",
    "        if '$' in chunk.find('span',{'class':'no-wrap'}).get_text().encode('ascii','ignore'):\n",
    "            salary_range = chunk.find('span',{'class':'no-wrap'}).get_text().encode('ascii','ignore')\n",
    "            #salary = re.findall(r'\\d+',salary_range.replace(',',''))\n",
    "\n",
    "            #if 'hour' in salary_range:\n",
    "            #    salary = salary * 2080\n",
    "            #elif 'day' in salary_range:\n",
    "            #    salary = salary * 260\n",
    "            #elif 'week' in salary_range:\n",
    "            #    salary = salary * 52\n",
    "            salaries.append(salary_range)\n",
    "\n",
    "            #salaries.append(np.mean([float(s) for s in salary]))\n",
    "    except:\n",
    "        salaries.append(np.nan)\n",
    "        \n",
    "    return salaries\n",
    "            \n",
    "def scrape_jobtype(chunk):\n",
    "    \n",
    "    job_types = []\n",
    "    \n",
    "    #get_extra = soup.findAll(\"div\", {\"data-tn-component\":\"organicJob\"})\n",
    "\n",
    "    #for chunk in soup:\n",
    "    try:\n",
    "        if '$' not in chunk.find('span',{'class':'no-wrap'}).get_text().encode('ascii','ignore'):\n",
    "            job_types.append(chunk.find('span',{'class':'no-wrap'}).get_text().encode('ascii','ignore'))\n",
    "        else:\n",
    "            job_types.append(np.nan)\n",
    "                        \n",
    "    except:\n",
    "        job_types.append(np.nan)\n",
    "        \n",
    "    return job_types\n",
    "            \n",
    "def scrape_summary(chunk):\n",
    "    \n",
    "    summaries = []\n",
    "\n",
    "    try:\n",
    "        summaries.append(chunk.find(\"span\", {\"id\":\"job_summary\"}).get_text())\n",
    "    except:\n",
    "        summaries.append(np.nan)\n",
    "            \n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......... Number of jobs scraped 10 .........\n",
      "......... Number of jobs scraped 20 .........\n",
      "......... Number of jobs scraped 30 .........\n",
      "......... Number of jobs scraped 40 .........\n",
      "......... Number of jobs scraped 50 .........\n",
      "......... Number of jobs scraped 60 .........\n",
      "......... Number of jobs scraped 70 .........\n",
      "......... Number of jobs scraped 80 .........\n",
      "......... Number of jobs scraped 90 .........\n",
      "......... Number of jobs scraped 100 .........\n",
      "......... Number of jobs scraped 110 .........\n",
      "......... Number of jobs scraped 120 .........\n",
      "......... Number of jobs scraped 130 .........\n",
      "......... Number of jobs scraped 140 .........\n",
      "......... Number of jobs scraped 150 .........\n",
      "......... Number of jobs scraped 160 .........\n",
      "......... Number of jobs scraped 170 .........\n",
      "......... Number of jobs scraped 180 .........\n",
      "......... Number of jobs scraped 190 .........\n",
      "......... Number of jobs scraped 200 .........\n",
      "......... Number of jobs scraped 210 .........\n",
      "......... Number of jobs scraped 220 .........\n",
      "......... Number of jobs scraped 230 .........\n",
      "......... Number of jobs scraped 240 .........\n",
      "......... Number of jobs scraped 250 .........\n",
      "......... Number of jobs scraped 260 .........\n",
      "......... Number of jobs scraped 270 .........\n",
      "......... Number of jobs scraped 280 .........\n",
      "......... Number of jobs scraped 290 .........\n",
      "......... Number of jobs scraped 300 .........\n",
      "......... Number of jobs scraped 310 .........\n",
      "......... Number of jobs scraped 320 .........\n",
      "......... Number of jobs scraped 330 .........\n",
      "......... Number of jobs scraped 340 .........\n",
      "......... Number of jobs scraped 350 .........\n",
      "......... Number of jobs scraped 360 .........\n",
      "......... Number of jobs scraped 370 .........\n",
      "......... Number of jobs scraped 380 .........\n",
      "......... Number of jobs scraped 390 .........\n",
      "......... Number of jobs scraped 400 .........\n",
      "......... Number of jobs scraped 410 .........\n",
      "......... Number of jobs scraped 420 .........\n",
      "......... Number of jobs scraped 430 .........\n",
      "......... Number of jobs scraped 440 .........\n",
      "......... Number of jobs scraped 450 .........\n",
      "......... Number of jobs scraped 460 .........\n",
      "......... Number of jobs scraped 470 .........\n",
      "......... Number of jobs scraped 480 .........\n",
      "......... Number of jobs scraped 490 .........\n",
      "......... Number of jobs scraped 500 .........\n",
      "......... Number of jobs scraped 510 .........\n",
      "......... Number of jobs scraped 520 .........\n",
      "......... Number of jobs scraped 530 .........\n",
      "......... Number of jobs scraped 540 .........\n",
      "......... Number of jobs scraped 550 .........\n",
      "......... Number of jobs scraped 560 .........\n",
      "......... Number of jobs scraped 570 .........\n",
      "......... Number of jobs scraped 580 .........\n",
      "......... Number of jobs scraped 590 .........\n",
      "......... Number of jobs scraped 600 .........\n"
     ]
    }
   ],
   "source": [
    "# find max pages\n",
    "\n",
    "url = 'https://www.indeed.com/jobs?q=data&start='\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "max_soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "max_page = int(max_soup.find('div',{'id':'searchCount'}).text.split(' ')[-1].replace(',',''))\n",
    "\n",
    "if max_page % 10 == 0:\n",
    "    max_page = max_page/10\n",
    "else:\n",
    "    max_page = max_page/10 + 1\n",
    "    \n",
    "\n",
    "job_titles_ = []\n",
    "companies_ = []\n",
    "locations_ = []\n",
    "reviews_ = []\n",
    "salaries_ = []\n",
    "job_types_ = []\n",
    "ratings_ = []\n",
    "\n",
    "for i in range(0,max_page,10):\n",
    "    \n",
    "    url = url+str(i)\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'lxml').find_all('div', {\"data-tn-component\":\"organicJob\"})\n",
    "    \n",
    "    for chunk in soup:\n",
    "    \n",
    "        job_titles = job_titles_.extend(scrape_title(chunk))\n",
    "        companies = companies_.extend(scrape_company(chunk))\n",
    "        locations = locations_.extend(scrape_location(chunk))\n",
    "        reviews = reviews_.extend(scrape_review(chunk))\n",
    "        salaries = salaries_.extend(scrape_salary(chunk))\n",
    "        job_types = job_types_.extend(scrape_jobtype(chunk))\n",
    "        ratings = ratings_.extend(scrape_rating(chunk))\n",
    "\n",
    "    print '......... Number of jobs scraped ' + str(i + 10)+ ' .........'\n",
    "\n",
    "    time.sleep(3)\n",
    "\n",
    "results = pd.DataFrame()\n",
    "\n",
    "results['job_titles'] = job_titles_\n",
    "results['companies'] = companies_\n",
    "results['locations'] = locations_\n",
    "results['reviews'] = reviews_\n",
    "results['salaries'] = salaries_\n",
    "results['job_types'] = job_types_\n",
    "results['ratings'] = ratings_\n",
    "\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
