{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:900px;background:#D9D9D9;border:1px solid black;text-align:left;padding:8px;\">\n",
    "\n",
    "\n",
    "\n",
    "<p>\n",
    "<b>Step 1:</b> URL = https://www.indeed.com\n",
    "<br><b>Step 2:</b> Keyword search parameter - \"Data\"\n",
    "<br><b>Step 3:</b> Location choice - USA (more data is avaliable)\n",
    "<br><b>Step 4:</b> Loop through 6 salary search parameter - \"25000\",\"35000\",\"50000\",\"65000\",\"80000\"\n",
    "<br><b>Step 5:</b> Each salary parameter scrape 100 pages (1000 jobs)\n",
    "<br><b>Step 6:</b> Scrape information: <b>job title, location, salary, ratings, reviews, summary</b>\n",
    "<br><b>Step 7:</b> Save data into pickle file\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:900px;background:#D9D9D9;border:1px solid black;text-align:left;padding:8px;\">\n",
    "\n",
    "\n",
    "\n",
    "<p>\n",
    "<b>Job title, Companies, Summaries, Locations</b>\n",
    "<br>1. Change data type from unicode to string. \n",
    "<br>2. Remove punctuations and special characters and lower case all text (except location)\n",
    "<br>\n",
    "<br><b> Location </b> \n",
    "<br>1. Replace with state abbrevations (e.g. AL, SF)\n",
    "<br>\n",
    "<br><b>Reviews, Ratings</b>\n",
    "<br>1. Fill NaN with 0.0\n",
    "<br><br><b>Salaries</b>\n",
    "<br>1. If information is avaiable, scale into annual salary. \n",
    "<br>2. If salary range is provided, obtain mean of range. \n",
    "<br>3. If information not avaliable, impute with estimated salary (what was input in search bar during scrapping) \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salary_func(x):\n",
    "    \n",
    "    \"\"\"Extracts salary information if avaliable - and scale them into annual salaries\n",
    "    if salary range is provided, take mean of range\"\"\"\n",
    "\n",
    "    try:\n",
    "        salary = np.mean([float(s) for s in re.findall(r'\\d+',a.replace(',','').replace('$',''))])\n",
    "\n",
    "        if 'hour' in x:\n",
    "            salary = salary * 2080\n",
    "        \n",
    "        elif 'day' in x:\n",
    "            salary = salary * 260\n",
    "            \n",
    "        elif 'week' in x:\n",
    "            salary = salary * 52\n",
    "        \n",
    "        \n",
    "    except:\n",
    "        salary = np.nan\n",
    "        \n",
    "    return salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_file = pd.DataFrame()\n",
    "\n",
    "for ind, filename in enumerate(glob.iglob('./indeed_jobs/results_25000_0.pickle')):   # Iterates through all pickle file in folder\n",
    "    \n",
    "    # Open each file as data\n",
    "    with open(filename) as inputfile:\n",
    "        \n",
    "        data = pickle.load(inputfile)  # Load pickle file\n",
    "\n",
    "        # Clean text features\n",
    "        \n",
    "        data.job_titles = data.job_titles.map(lambda x : x.encode('ascii','ignore')\\\n",
    "                                              .replace('\\n','')\\\n",
    "                                              .translate(None, string.punctuation)\\\n",
    "                                              .lower())\n",
    "        \n",
    "        data.companies = data.companies.map(lambda x : x.encode('ascii','ignore')\\\n",
    "                                              .replace('\\n','')\\\n",
    "                                              .translate(None, string.punctuation)\\\n",
    "                                              .lower())\n",
    "        \n",
    "        data.locations = data.locations.map(lambda x : x.encode('ascii','ignore')\\\n",
    "                                              .replace('\\n','')\\\n",
    "                                              .translate(None, string.punctuation))\n",
    "        \n",
    "        #data.summaries = data.summaries.map(lambda x : x.encode('ascii','ignore')\\\n",
    "         #                                     .replace('\\n','')\\\n",
    "          #                                    .translate(None, string.punctuation)\n",
    "           #                                   .lower())\n",
    "\n",
    "        # Impute NaN values with 0.0\n",
    "        \n",
    "        data.reviews = data.reviews.fillna(0.0)\n",
    "\n",
    "        data.ratings = data.ratings.fillna(0.0)\n",
    "        \n",
    "        # Extract salaries\n",
    "\n",
    "        data.salaries = data.salaries.map(salary_func)\n",
    "\n",
    "        # If exact salary is not provided, impute with salary that was searched on\n",
    "        \n",
    "        base_salary = [25000,35000,50000,65000,80000]\n",
    "\n",
    "        for indx, base in enumerate(base_salary):\n",
    "\n",
    "            if ind == indx:\n",
    "                \n",
    "                data.salaries = data.salaries.fillna(base)\n",
    "\n",
    "        \n",
    "    jobs_file = jobs_file.append(data)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:900px;background:#D9D9D9;border:1px solid black;text-align:left;padding:8px;\">\n",
    "\n",
    "<p>\n",
    "Check for NaN values\n",
    "<br>\n",
    "Investigate outliers\n",
    "<br>\n",
    "Visualise correlation matrix and distributions\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 980 entries, 0 to 979\n",
      "Data columns (total 7 columns):\n",
      "job_titles    980 non-null object\n",
      "companies     980 non-null object\n",
      "locations     980 non-null object\n",
      "reviews       980 non-null float64\n",
      "salaries      980 non-null float64\n",
      "ratings       980 non-null float64\n",
      "summaries     0 non-null float64\n",
      "dtypes: float64(4), object(3)\n",
      "memory usage: 61.2+ KB\n"
     ]
    }
   ],
   "source": [
    "jobs_file.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_titles</th>\n",
       "      <th>companies</th>\n",
       "      <th>locations</th>\n",
       "      <th>reviews</th>\n",
       "      <th>salaries</th>\n",
       "      <th>ratings</th>\n",
       "      <th>summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data conversion specialist</td>\n",
       "      <td>paylocity</td>\n",
       "      <td>Remote</td>\n",
       "      <td>49.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>41.4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data analytics intern  undergraduate  masters ...</td>\n",
       "      <td>att</td>\n",
       "      <td>California</td>\n",
       "      <td>19490.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>44.4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data analytics coop  undergraduate  masters de...</td>\n",
       "      <td>att</td>\n",
       "      <td>California</td>\n",
       "      <td>19490.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>44.4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vice president  data integration manager  firm...</td>\n",
       "      <td>jp morgan chase</td>\n",
       "      <td>Jersey City NJ 07310 Downtown area</td>\n",
       "      <td>14857.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>44.4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jr research analyst</td>\n",
       "      <td>valuepenguin</td>\n",
       "      <td>New York NY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          job_titles        companies  \\\n",
       "0                         data conversion specialist        paylocity   \n",
       "1  data analytics intern  undergraduate  masters ...              att   \n",
       "2  data analytics coop  undergraduate  masters de...              att   \n",
       "3  vice president  data integration manager  firm...  jp morgan chase   \n",
       "4                                jr research analyst     valuepenguin   \n",
       "\n",
       "                            locations  reviews  salaries  ratings  summaries  \n",
       "0                              Remote     49.0   25000.0     41.4        NaN  \n",
       "1                          California  19490.0   25000.0     44.4        NaN  \n",
       "2                          California  19490.0   25000.0     44.4        NaN  \n",
       "3  Jersey City NJ 07310 Downtown area  14857.0   25000.0     44.4        NaN  \n",
       "4                         New York NY      0.0   25000.0      0.0        NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:900px;background:#D9D9D9;border:1px solid black;text-align:left;padding:8px;\">\n",
    "\n",
    "\n",
    "\n",
    "<p>\n",
    "Based on the companies, I will engineer a new binary column on whether the company is in the Forbes US 500 list (2016 & 2017)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fortune16 = pd.read_csv('./Project_4_Salary_Predictions/fortune500_16.csv')['Company Name']\n",
    "fortune17 = pd.read_csv('./Project_4_Salary_Predictions/fortune500_17.csv')['Company Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fortune = pd.concat([fortune16,fortune17]).drop_duplicates().reset_index(drop=True).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:900px;background:#D9D9D9;border:1px solid black;text-align:left;padding:8px;\">\n",
    "\n",
    "\n",
    "\n",
    "<p>\n",
    "Scale numerical data - <b>Reviews, Ratings</b>\n",
    "<Br>\n",
    "Get dummy variables for categorical - <b>Locations</b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=2,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:900px;background:#D9D9D9;border:1px solid black;text-align:left;padding:8px;\">\n",
    "\n",
    "\n",
    "\n",
    "<p>\n",
    "<b>Step 1:</b> Add customized stopwords \n",
    "<br><b>Step 2:</b> Count vectorizer on job_titles, summaries and companies\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(stopwords = stopwords)\n",
    "cvec.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df  = pd.DataFrame(cvec.transform([FEATURE]).todense(),\n",
    "             columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:900px;background:#D9D9D9;border:1px solid black;text-align:left;padding:8px;\">\n",
    "\n",
    "\n",
    "\n",
    "<p>\n",
    "<b>Variance Inflation Factor</b> on numerical data to check for multi-colinearity\n",
    "<br><b>ANOVA Test</b> on binary data\n",
    "<br>Since there is little data avaliable, I will use <b>PCA</b> or <b>LDA</b> to reduce dimensionality\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variance Inflation Factor\n",
    "\n",
    "class VIF(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Variance Inflation Factor\"\"\"\n",
    "    \n",
    "    def __init__(self,threshold=5):\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    \n",
    "    def vif(self,df):\n",
    "        \n",
    "        vif = [variance_inflation_factor(df.iloc[:,:-4].values,i) \\\n",
    "               for i in range(df.iloc[:,:-4].shape[1])]\n",
    "        \n",
    "        vif_df = pd.DataFrame(df.iloc[:,:-4].columns,columns=['Features'])\n",
    "        vif_df['VIF'] = vif    # VIF values in dataframe\n",
    "        \n",
    "        remove_col = list(vif_df[vif_df['VIF']>self.threshold]['Features'])   # Choose only features with VIF < 5\n",
    "        selected_df = df.drop(remove_col,axis=1)\n",
    "        \n",
    "        return selected_df\n",
    "    \n",
    "    def transform(self, df, *args):\n",
    "        \n",
    "        selected_df = self.vif(df)\n",
    "        \n",
    "        return selected_df\n",
    "\n",
    "\n",
    "    def fit(self, df, *args):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Models\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Learning Curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title = 'Learning Curve Random Forest'\n",
    "plot_learning_curve(ESTIMATOR_GS.best_estimator_, title, X, y, ylim=None, cv=None,\\\n",
    "                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
