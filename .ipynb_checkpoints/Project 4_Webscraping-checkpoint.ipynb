{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.set(rc={\"figure.figsize\": (15, 8)})\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:900px;background:#F9EECF;border:1px solid black;text-align:left;padding:8px;\">\n",
    "\n",
    "\n",
    "\n",
    "<p>\n",
    "<span style=\"font-size:14pt\"><b>Scraping from indeed.com</span></b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scraping from indeed.com\n",
    "\n",
    "country_url = {\"SG\":\"https://www.indeed.com.sg/jobs\",\n",
    "       \"US\":\"https://www.indeed.com/jobs\",\n",
    "       \"MY\":\"https://www.indeed.com.my/jobs\",\n",
    "       \"HK\":\"https://www.indeed.hk/jobs\",\n",
    "       \"ID\":\"https://id.indeed.com/lowongan-kerja\"\n",
    "       \n",
    "       }\n",
    "\n",
    "countries = {\"SG\":\"Singapore\",\"US\":\"United States\",\"MY\":\"Malaysia\",\"HK\":\"Hong Kong\",\"ID\":'Indonesia'}\n",
    "\n",
    "target_cities= {'US':\n",
    "                      ['New York', 'Chicago', 'San Francisco', 'Austin', 'Seattle',\n",
    "                  'Los Angeles', 'Philadelphia', 'Atlanta', 'Dallas',\n",
    "                  'Pittsburgh', 'Portland', 'Phoenix', 'Denver', 'Houston','Miami'],\n",
    "                'SG':[\"Singapore\"],\n",
    "                'MY':['Kuala Lumpur','Johor Bahru','Shah Alam'],\n",
    "                'HK':['Hong Kong'],\n",
    "                'ID':['Jakarta','Batam','Surabaya']\n",
    "                }\n",
    "\n",
    "job_titles = ['data scientist', 'data analyst','chief data officer','chief information officer',\\\n",
    "              'data engineer','business intelligence','artifical intelligence','machine learning'\\\n",
    "             'data consultant','marketing analyst','marketing intelligence','deep learning',\\\n",
    "             'chatbot','system analyst','data crawling','data entry','data administrator',\\\n",
    "             'nlp','data analytics','data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://www.indeed.com/jobs?q=data&start=80'\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_title(soup): \n",
    "    \n",
    "    job_titles = []\n",
    "    \n",
    "    try:\n",
    "        for title in soup.findAll('h2',{'class':'jobtitle'}):\n",
    "            job_titles.append(title.text)\n",
    "    except:\n",
    "        job_titles.append(np.nan)\n",
    "        \n",
    "    return job_titles\n",
    "\n",
    "def scrape_company(soup):\n",
    "    \n",
    "    companies = []\n",
    "    \n",
    "    try:\n",
    "        for company in soup.findAll('span',{'class':'company'}):\n",
    "            companies.append(company.text)\n",
    "    except:\n",
    "        companies.append(np.nan)\n",
    "        \n",
    "    return companies\n",
    "\n",
    "def scrape_location(soup):\n",
    "    \n",
    "    locations = []\n",
    "    \n",
    "    try:\n",
    "        for location in soup.findAll('span',{'class':'location'}):\n",
    "            locations.append(location.text)\n",
    "    except:\n",
    "        locations.append(np.nan)\n",
    "        \n",
    "    return locations\n",
    "\n",
    "def scrape_review(soup):\n",
    "    \n",
    "    reviews = []\n",
    "    \n",
    "    try:\n",
    "        for review in soup.findAll('span',{'class':'slNoUnderline'}):\n",
    "            review_ = int(review.text.replace(' reviews','').replace(',',''))\n",
    "            reviews.append(review_)\n",
    "    except:\n",
    "        reviews.append(np.nan)\n",
    "        \n",
    "    return reviews\n",
    "\n",
    "def scrape_rating(soup):\n",
    "    \n",
    "    ratings = []\n",
    "    \n",
    "    try:\n",
    "        for rating in soup.findAll('span',{'class':'rating'}):\n",
    "            ratings.append(float(rating.get('style').replace('width:','').replace('px','')))\n",
    "    except:\n",
    "        ratings.append(np.nan)\n",
    "        \n",
    "    return ratings\n",
    "\n",
    "def scrape_salary(soup):\n",
    "    \n",
    "    salaries = []\n",
    "    \n",
    "    get_extra = soup.findAll(\"div\", {\"data-tn-component\":\"organicJob\"})\n",
    "\n",
    "    for chunk in get_extra:\n",
    "\n",
    "        try:\n",
    "            if '$' in chunk.find('span',{'class':'no-wrap'}).text.encode('ascii','ignore'):\n",
    "                salary_range = chunk.find('span',{'class':'no-wrap'}).text.encode('ascii','ignore')\n",
    "                salary = re.findall(r'\\d+',salary_range.replace(',',''))\n",
    "\n",
    "                if 'hour' in salary_range:\n",
    "                    salary = salary * 2080\n",
    "                elif 'day' in salary_range:\n",
    "                    salary = salary * 260\n",
    "                elif 'week' in salary_range:\n",
    "                    salary = salary * 52\n",
    "\n",
    "                salaries.append(np.mean([float(s) for s in salary]))\n",
    "        except:\n",
    "            salaries.append(np.nan)\n",
    "        \n",
    "    return salaries\n",
    "            \n",
    "def scrape_jobtype(soup):\n",
    "    \n",
    "    job_types = []\n",
    "    \n",
    "    get_extra = soup.findAll(\"div\", {\"data-tn-component\":\"organicJob\"})\n",
    "\n",
    "    for chunk in get_extra:\n",
    "        try:\n",
    "            if '$' not in chunk.find('span',{'class':'no-wrap'}).text.encode('ascii','ignore'):\n",
    "                job_types.append(chunk.find('span',{'class':'no-wrap'}).text.encode('ascii','ignore'))\n",
    "                        \n",
    "        except:\n",
    "            job_types.append(np.nan)\n",
    "        \n",
    "    return job_types\n",
    "            \n",
    "def scrape_summary(soup):\n",
    "    \n",
    "    summaries = []\n",
    "\n",
    "    try:\n",
    "        summaries.append(soup.find(\"span\", {\"id\":\"job_summary\"}).text)\n",
    "    except:\n",
    "        summaries.append(np.nan)\n",
    "            \n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find max pages\n",
    "\n",
    "max_page = int(soup.find('div',{'id':'searchCount'}).text.split(' ')[-1].replace(',',''))\n",
    "\n",
    "if max_page % 10 == 0:\n",
    "    max_page = max_page/10\n",
    "else:\n",
    "    max_page = max_page/10 + 1\n",
    "\n",
    "for i in range(0,max_page,10):\n",
    "    url = 'https://www.indeed.com/jobs?q=data&start='+str(i)\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-222-ba7dcb35df16>, line 124)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-222-ba7dcb35df16>\"\u001b[0;36m, line \u001b[0;32m124\u001b[0m\n\u001b[0;31m    def\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for \n",
    "\n",
    "url = 'https://www.indeed.com/jobs?q=data&start=80'\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "job_titles = scrape_title(soup)\n",
    "companies = scrape_company(soup)\n",
    "locations = scrape_location(soup)\n",
    "reviews = scrape_review(soup)\n",
    "salaries = scrape_salary(soup)\n",
    "job_types = scrape_jobtype(soup)\n",
    "ratings = scrape_rating(soup)\n",
    "\n",
    "results = pd.DataFrame()\n",
    "\n",
    "results['job_title'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scrape_title' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-223-c87bc540b698>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjob_titles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcompanies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_company\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlocations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mreviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_review\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msalaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_salary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scrape_title' is not defined"
     ]
    }
   ],
   "source": [
    "job_titles = scrape_title(soup)\n",
    "companies = scrape_company(soup)\n",
    "locations = scrape_location(soup)\n",
    "reviews = scrape_review(soup)\n",
    "salaries = scrape_salary(soup)\n",
    "job_types = scrape_jobtype(soup)\n",
    "ratings = scrape_rating(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
