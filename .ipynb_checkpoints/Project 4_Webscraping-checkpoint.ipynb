{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.set(rc={\"figure.figsize\": (15, 8)})\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:900px;background:#F9EECF;border:1px solid black;text-align:left;padding:8px;\">\n",
    "\n",
    "\n",
    "\n",
    "<p>\n",
    "<span style=\"font-size:14pt\"><b>Scraping from indeed.com</span></b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scraping from indeed.com\n",
    "\n",
    "country_url = {\"SG\":\"https://www.indeed.com.sg/jobs\",\n",
    "       \"US\":\"https://www.indeed.com/jobs\",\n",
    "       \"MY\":\"https://www.indeed.com.my/jobs\",\n",
    "       \"HK\":\"https://www.indeed.hk/jobs\",\n",
    "       \"ID\":\"https://id.indeed.com/lowongan-kerja\"\n",
    "       \n",
    "       }\n",
    "\n",
    "countries = {\"SG\":\"Singapore\",\"US\":\"United States\",\"MY\":\"Malaysia\",\"HK\":\"Hong Kong\",\"ID\":'Indonesia'}\n",
    "\n",
    "target_cities= {'US':\n",
    "                      ['New York', 'Chicago', 'San Francisco', 'Austin', 'Seattle',\n",
    "                  'Los Angeles', 'Philadelphia', 'Atlanta', 'Dallas',\n",
    "                  'Pittsburgh', 'Portland', 'Phoenix', 'Denver', 'Houston','Miami'],\n",
    "                'SG':[\"Singapore\"],\n",
    "                'MY':['Kuala Lumpur','Johor Bahru','Shah Alam'],\n",
    "                'HK':['Hong Kong'],\n",
    "                'ID':['Jakarta','Batam','Surabaya']\n",
    "                }\n",
    "\n",
    "job_titles = ['data scientist', 'data analyst','chief data officer','chief information officer',\\\n",
    "              'data engineer','business intelligence','artifical intelligence','machine learning'\\\n",
    "             'data consultant','marketing analyst','marketing intelligence','deep learning',\\\n",
    "             'chatbot','system analyst','data crawling','data entry','data administrator',\\\n",
    "             'nlp','data analytics','data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_title(chunk): \n",
    "    \n",
    "    job_titles = []\n",
    "    \n",
    "    try:\n",
    "        title = chunk.find('h2',{'class':'jobtitle'}).get_text()\n",
    "        job_titles.append(title)\n",
    "    except:\n",
    "        job_titles.append(np.nan)\n",
    "        \n",
    "    return job_titles\n",
    "\n",
    "def scrape_company(chunk):\n",
    "    \n",
    "    companies = []\n",
    "    \n",
    "    try:\n",
    "        company = chunk.find('span',{'class':'company'}).get_text()\n",
    "        companies.append(company)\n",
    "    except:\n",
    "        companies.append(np.nan)\n",
    "        \n",
    "    return companies\n",
    "\n",
    "def scrape_location(chunk):\n",
    "    \n",
    "    locations = []\n",
    "    \n",
    "    try:\n",
    "        location = chunk.find('span',{'class':'location'}).get_text()\n",
    "        locations.append(location)\n",
    "    except:\n",
    "        locations.append(np.nan)\n",
    "        \n",
    "    return locations\n",
    "\n",
    "def scrape_review(chunk):\n",
    "    \n",
    "    reviews = []\n",
    "    \n",
    "    try:\n",
    "        review = chunk.find('span',{'class':'slNoUnderline'}).get_text()\n",
    "        review_ = int(review.replace(' reviews','').replace(',',''))\n",
    "        reviews.append(review_)\n",
    "    except:\n",
    "        reviews.append(np.nan)\n",
    "        \n",
    "    return reviews\n",
    "\n",
    "def scrape_rating(chunk):\n",
    "    \n",
    "    ratings = []\n",
    "    \n",
    "    try:\n",
    "        rating = chunk.find('span',{'class':'rating'})\n",
    "        ratings.append(float(rating.get('style').replace('width:','').replace('px','')))\n",
    "        \n",
    "    except:\n",
    "        ratings.append(np.nan)\n",
    "        \n",
    "    return ratings\n",
    "\n",
    "def scrape_salary(chunk):\n",
    "    \n",
    "    salaries = []\n",
    "    \n",
    "    #get_extra = soup.findAll(\"div\", {\"data-tn-component\":\"organicJob\"})\n",
    "\n",
    "    #for chunk in get_extra:\n",
    "\n",
    "    try:\n",
    "        if '$' in chunk.find('span',{'class':'no-wrap'}).get_text().encode('ascii','ignore'):\n",
    "            salary_range = chunk.find('span',{'class':'no-wrap'}).get_text().encode('ascii','ignore')\n",
    "            #salary = re.findall(r'\\d+',salary_range.replace(',',''))\n",
    "\n",
    "            #if 'hour' in salary_range:\n",
    "            #    salary = salary * 2080\n",
    "            #elif 'day' in salary_range:\n",
    "            #    salary = salary * 260\n",
    "            #elif 'week' in salary_range:\n",
    "            #    salary = salary * 52\n",
    "            salaries.append(salary_range)\n",
    "\n",
    "            #salaries.append(np.mean([float(s) for s in salary]))\n",
    "    except:\n",
    "        salaries.append(np.nan)\n",
    "        \n",
    "    return salaries\n",
    "            \n",
    "def scrape_jobtype(chunk):\n",
    "    \n",
    "    job_types = []\n",
    "    \n",
    "    #get_extra = soup.findAll(\"div\", {\"data-tn-component\":\"organicJob\"})\n",
    "\n",
    "    #for chunk in soup:\n",
    "    try:\n",
    "        if '$' not in chunk.find('span',{'class':'no-wrap'}).get_text().encode('ascii','ignore'):\n",
    "            job_types.append(chunk.find('span',{'class':'no-wrap'}).get_text().encode('ascii','ignore'))\n",
    "        else:\n",
    "            job_types.append(np.nan)\n",
    "                        \n",
    "    except:\n",
    "        job_types.append(np.nan)\n",
    "        \n",
    "    return job_types\n",
    "            \n",
    "def scrape_summary(chunk):\n",
    "    \n",
    "    summaries = []\n",
    "\n",
    "    try:\n",
    "        summaries.append(chunk.find(\"span\", {\"id\":\"job_summary\"}).get_text())\n",
    "    except:\n",
    "        summaries.append(np.nan)\n",
    "            \n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......... Number of jobs scraped 10 .........\n",
      "......... Number of jobs scraped 20 .........\n",
      "......... Number of jobs scraped 30 .........\n",
      "......... Number of jobs scraped 40 .........\n",
      "......... Number of jobs scraped 50 .........\n",
      "......... Number of jobs scraped 60 .........\n",
      "......... Number of jobs scraped 70 .........\n",
      "......... Number of jobs scraped 80 .........\n",
      "......... Number of jobs scraped 90 .........\n",
      "......... Number of jobs scraped 100 .........\n",
      "......... Number of jobs scraped 110 .........\n",
      "......... Number of jobs scraped 120 .........\n",
      "......... Number of jobs scraped 130 .........\n",
      "......... Number of jobs scraped 140 .........\n",
      "......... Number of jobs scraped 150 .........\n",
      "......... Number of jobs scraped 160 .........\n",
      "......... Number of jobs scraped 170 .........\n",
      "......... Number of jobs scraped 180 .........\n",
      "......... Number of jobs scraped 190 .........\n",
      "......... Number of jobs scraped 200 .........\n",
      "......... Number of jobs scraped 210 .........\n",
      "......... Number of jobs scraped 220 .........\n",
      "......... Number of jobs scraped 230 .........\n",
      "......... Number of jobs scraped 240 .........\n",
      "......... Number of jobs scraped 250 .........\n",
      "......... Number of jobs scraped 260 .........\n",
      "......... Number of jobs scraped 270 .........\n",
      "......... Number of jobs scraped 280 .........\n",
      "......... Number of jobs scraped 290 .........\n",
      "......... Number of jobs scraped 300 .........\n",
      "......... Number of jobs scraped 310 .........\n",
      "......... Number of jobs scraped 320 .........\n",
      "......... Number of jobs scraped 330 .........\n",
      "......... Number of jobs scraped 340 .........\n",
      "......... Number of jobs scraped 350 .........\n",
      "......... Number of jobs scraped 360 .........\n",
      "......... Number of jobs scraped 370 .........\n",
      "......... Number of jobs scraped 380 .........\n",
      "......... Number of jobs scraped 390 .........\n",
      "......... Number of jobs scraped 400 .........\n",
      "......... Number of jobs scraped 410 .........\n",
      "......... Number of jobs scraped 420 .........\n",
      "......... Number of jobs scraped 430 .........\n",
      "......... Number of jobs scraped 440 .........\n",
      "......... Number of jobs scraped 450 .........\n",
      "......... Number of jobs scraped 460 .........\n",
      "......... Number of jobs scraped 470 .........\n",
      "......... Number of jobs scraped 480 .........\n",
      "......... Number of jobs scraped 490 .........\n",
      "......... Number of jobs scraped 500 .........\n",
      "......... Number of jobs scraped 510 .........\n",
      "......... Number of jobs scraped 520 .........\n",
      "......... Number of jobs scraped 530 .........\n",
      "......... Number of jobs scraped 540 .........\n",
      "......... Number of jobs scraped 550 .........\n",
      "......... Number of jobs scraped 560 .........\n",
      "......... Number of jobs scraped 570 .........\n",
      "......... Number of jobs scraped 580 .........\n",
      "......... Number of jobs scraped 590 .........\n",
      "......... Number of jobs scraped 600 .........\n",
      "......... Number of jobs scraped 610 .........\n",
      "......... Number of jobs scraped 620 .........\n",
      "......... Number of jobs scraped 630 .........\n",
      "......... Number of jobs scraped 640 .........\n",
      "......... Number of jobs scraped 650 .........\n",
      "......... Number of jobs scraped 660 .........\n",
      "......... Number of jobs scraped 670 .........\n",
      "......... Number of jobs scraped 680 .........\n",
      "......... Number of jobs scraped 690 .........\n",
      "......... Number of jobs scraped 700 .........\n",
      "......... Number of jobs scraped 710 .........\n",
      "......... Number of jobs scraped 720 .........\n",
      "......... Number of jobs scraped 730 .........\n",
      "......... Number of jobs scraped 740 .........\n",
      "......... Number of jobs scraped 750 .........\n",
      "......... Number of jobs scraped 760 .........\n",
      "......... Number of jobs scraped 770 .........\n",
      "......... Number of jobs scraped 780 .........\n",
      "......... Number of jobs scraped 790 .........\n",
      "......... Number of jobs scraped 800 .........\n",
      "......... Number of jobs scraped 810 .........\n",
      "......... Number of jobs scraped 820 .........\n",
      "......... Number of jobs scraped 830 .........\n",
      "......... Number of jobs scraped 840 .........\n",
      "......... Number of jobs scraped 850 .........\n",
      "......... Number of jobs scraped 860 .........\n",
      "......... Number of jobs scraped 870 .........\n",
      "......... Number of jobs scraped 880 .........\n",
      "......... Number of jobs scraped 890 .........\n",
      "......... Number of jobs scraped 900 .........\n",
      "......... Number of jobs scraped 910 .........\n",
      "......... Number of jobs scraped 920 .........\n",
      "......... Number of jobs scraped 930 .........\n",
      "......... Number of jobs scraped 940 .........\n",
      "......... Number of jobs scraped 950 .........\n",
      "......... Number of jobs scraped 960 .........\n",
      "......... Number of jobs scraped 970 .........\n",
      "......... Number of jobs scraped 980 .........\n",
      "......... Number of jobs scraped 990 .........\n",
      "......... Number of jobs scraped 1000 .........\n",
      "......... Number of jobs scraped 1010 .........\n",
      "......... Number of jobs scraped 1020 .........\n",
      "......... Number of jobs scraped 1030 .........\n",
      "......... Number of jobs scraped 1040 .........\n",
      "......... Number of jobs scraped 1050 .........\n",
      "......... Number of jobs scraped 1060 .........\n",
      "......... Number of jobs scraped 1070 .........\n",
      "......... Number of jobs scraped 1080 .........\n",
      "......... Number of jobs scraped 1090 .........\n",
      "......... Number of jobs scraped 1100 .........\n",
      "......... Number of jobs scraped 1110 .........\n",
      "......... Number of jobs scraped 1120 .........\n",
      "......... Number of jobs scraped 1130 .........\n",
      "......... Number of jobs scraped 1140 .........\n",
      "......... Number of jobs scraped 1150 .........\n",
      "......... Number of jobs scraped 1160 .........\n",
      "......... Number of jobs scraped 1170 .........\n",
      "......... Number of jobs scraped 1180 .........\n",
      "......... Number of jobs scraped 1190 .........\n",
      "......... Number of jobs scraped 1200 .........\n",
      "......... Number of jobs scraped 1210 .........\n",
      "......... Number of jobs scraped 1220 .........\n",
      "......... Number of jobs scraped 1230 .........\n",
      "......... Number of jobs scraped 1240 .........\n",
      "......... Number of jobs scraped 1250 .........\n",
      "......... Number of jobs scraped 1260 .........\n",
      "......... Number of jobs scraped 1270 .........\n",
      "......... Number of jobs scraped 1280 .........\n",
      "......... Number of jobs scraped 1290 .........\n",
      "......... Number of jobs scraped 1300 .........\n",
      "......... Number of jobs scraped 1310 .........\n",
      "......... Number of jobs scraped 1320 .........\n",
      "......... Number of jobs scraped 1330 .........\n",
      "......... Number of jobs scraped 1340 .........\n",
      "......... Number of jobs scraped 1350 .........\n",
      "......... Number of jobs scraped 1360 .........\n",
      "......... Number of jobs scraped 1370 .........\n",
      "......... Number of jobs scraped 1380 .........\n",
      "......... Number of jobs scraped 1390 .........\n",
      "......... Number of jobs scraped 1400 .........\n",
      "......... Number of jobs scraped 1410 .........\n",
      "......... Number of jobs scraped 1420 .........\n",
      "......... Number of jobs scraped 1430 .........\n",
      "......... Number of jobs scraped 1440 .........\n",
      "......... Number of jobs scraped 1450 .........\n",
      "......... Number of jobs scraped 1460 .........\n",
      "......... Number of jobs scraped 1470 .........\n",
      "......... Number of jobs scraped 1480 .........\n",
      "......... Number of jobs scraped 1490 .........\n",
      "......... Number of jobs scraped 1500 .........\n",
      "......... Number of jobs scraped 1510 .........\n",
      "......... Number of jobs scraped 1520 .........\n",
      "......... Number of jobs scraped 1530 .........\n",
      "......... Number of jobs scraped 1540 .........\n",
      "......... Number of jobs scraped 1550 .........\n",
      "......... Number of jobs scraped 1560 .........\n",
      "......... Number of jobs scraped 1570 .........\n",
      "......... Number of jobs scraped 1580 .........\n",
      "......... Number of jobs scraped 1590 .........\n",
      "......... Number of jobs scraped 1600 .........\n",
      "......... Number of jobs scraped 1610 .........\n",
      "......... Number of jobs scraped 1620 .........\n",
      "......... Number of jobs scraped 1630 .........\n",
      "......... Number of jobs scraped 1640 .........\n",
      "......... Number of jobs scraped 1650 .........\n",
      "......... Number of jobs scraped 1660 .........\n",
      "......... Number of jobs scraped 1670 .........\n",
      "......... Number of jobs scraped 1680 .........\n",
      "......... Number of jobs scraped 1690 .........\n",
      "......... Number of jobs scraped 1700 .........\n",
      "......... Number of jobs scraped 1710 .........\n",
      "......... Number of jobs scraped 1720 .........\n",
      "......... Number of jobs scraped 1730 .........\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......... Number of jobs scraped 1740 .........\n",
      "......... Number of jobs scraped 1750 .........\n",
      "......... Number of jobs scraped 1760 .........\n",
      "......... Number of jobs scraped 1770 .........\n",
      "......... Number of jobs scraped 1780 .........\n",
      "......... Number of jobs scraped 1790 .........\n",
      "......... Number of jobs scraped 1800 .........\n",
      "......... Number of jobs scraped 1810 .........\n",
      "......... Number of jobs scraped 1820 .........\n",
      "......... Number of jobs scraped 1830 .........\n",
      "......... Number of jobs scraped 1840 .........\n",
      "......... Number of jobs scraped 1850 .........\n",
      "......... Number of jobs scraped 1860 .........\n",
      "......... Number of jobs scraped 1870 .........\n",
      "......... Number of jobs scraped 1880 .........\n",
      "......... Number of jobs scraped 1890 .........\n",
      "......... Number of jobs scraped 1900 .........\n",
      "......... Number of jobs scraped 1910 .........\n",
      "......... Number of jobs scraped 1920 .........\n",
      "......... Number of jobs scraped 1930 .........\n",
      "......... Number of jobs scraped 1940 .........\n",
      "......... Number of jobs scraped 1950 .........\n",
      "......... Number of jobs scraped 1960 .........\n",
      "......... Number of jobs scraped 1970 .........\n",
      "......... Number of jobs scraped 1980 .........\n",
      "......... Number of jobs scraped 1990 .........\n",
      "......... Number of jobs scraped 2000 .........\n",
      "......... Number of jobs scraped 2010 .........\n",
      "......... Number of jobs scraped 2020 .........\n",
      "......... Number of jobs scraped 2030 .........\n",
      "......... Number of jobs scraped 2040 .........\n",
      "......... Number of jobs scraped 2050 .........\n",
      "......... Number of jobs scraped 2060 .........\n",
      "......... Number of jobs scraped 2070 .........\n",
      "......... Number of jobs scraped 2080 .........\n",
      "......... Number of jobs scraped 2090 .........\n",
      "......... Number of jobs scraped 2100 .........\n",
      "......... Number of jobs scraped 2110 .........\n",
      "......... Number of jobs scraped 2120 .........\n",
      "......... Number of jobs scraped 2130 .........\n",
      "......... Number of jobs scraped 2140 .........\n",
      "......... Number of jobs scraped 2150 .........\n",
      "......... Number of jobs scraped 2160 .........\n",
      "......... Number of jobs scraped 2170 .........\n",
      "......... Number of jobs scraped 2180 .........\n",
      "......... Number of jobs scraped 2190 .........\n",
      "......... Number of jobs scraped 2200 .........\n",
      "......... Number of jobs scraped 2210 .........\n",
      "......... Number of jobs scraped 2220 .........\n",
      "......... Number of jobs scraped 2230 .........\n",
      "......... Number of jobs scraped 2240 .........\n",
      "......... Number of jobs scraped 2250 .........\n",
      "......... Number of jobs scraped 2260 .........\n",
      "......... Number of jobs scraped 2270 .........\n",
      "......... Number of jobs scraped 2280 .........\n",
      "......... Number of jobs scraped 2290 .........\n",
      "......... Number of jobs scraped 2300 .........\n",
      "......... Number of jobs scraped 2310 .........\n",
      "......... Number of jobs scraped 2320 .........\n",
      "......... Number of jobs scraped 2330 .........\n",
      "......... Number of jobs scraped 2340 .........\n",
      "......... Number of jobs scraped 2350 .........\n",
      "......... Number of jobs scraped 2360 .........\n",
      "......... Number of jobs scraped 2370 .........\n",
      "......... Number of jobs scraped 2380 .........\n",
      "......... Number of jobs scraped 2390 .........\n",
      "......... Number of jobs scraped 2400 .........\n",
      "......... Number of jobs scraped 2410 .........\n",
      "......... Number of jobs scraped 2420 .........\n",
      "......... Number of jobs scraped 2430 .........\n",
      "......... Number of jobs scraped 2440 .........\n",
      "......... Number of jobs scraped 2450 .........\n",
      "......... Number of jobs scraped 2460 .........\n",
      "......... Number of jobs scraped 2470 .........\n",
      "......... Number of jobs scraped 2480 .........\n",
      "......... Number of jobs scraped 2490 .........\n",
      "......... Number of jobs scraped 2500 .........\n",
      "......... Number of jobs scraped 2510 .........\n",
      "......... Number of jobs scraped 2520 .........\n",
      "......... Number of jobs scraped 2530 .........\n",
      "......... Number of jobs scraped 2540 .........\n",
      "......... Number of jobs scraped 2550 .........\n",
      "......... Number of jobs scraped 2560 .........\n",
      "......... Number of jobs scraped 2570 .........\n",
      "......... Number of jobs scraped 2580 .........\n",
      "......... Number of jobs scraped 2590 .........\n",
      "......... Number of jobs scraped 2600 .........\n",
      "......... Number of jobs scraped 2610 .........\n",
      "......... Number of jobs scraped 2620 .........\n",
      "......... Number of jobs scraped 2630 .........\n",
      "......... Number of jobs scraped 2640 .........\n",
      "......... Number of jobs scraped 2650 .........\n",
      "......... Number of jobs scraped 2660 .........\n",
      "......... Number of jobs scraped 2670 .........\n",
      "......... Number of jobs scraped 2680 .........\n",
      "......... Number of jobs scraped 2690 .........\n",
      "......... Number of jobs scraped 2700 .........\n",
      "......... Number of jobs scraped 2710 .........\n",
      "......... Number of jobs scraped 2720 .........\n",
      "......... Number of jobs scraped 2730 .........\n",
      "......... Number of jobs scraped 2740 .........\n",
      "......... Number of jobs scraped 2750 .........\n",
      "......... Number of jobs scraped 2760 .........\n",
      "......... Number of jobs scraped 2770 .........\n",
      "......... Number of jobs scraped 2780 .........\n",
      "......... Number of jobs scraped 2790 .........\n",
      "......... Number of jobs scraped 2800 .........\n",
      "......... Number of jobs scraped 2810 .........\n",
      "......... Number of jobs scraped 2820 .........\n",
      "......... Number of jobs scraped 2830 .........\n",
      "......... Number of jobs scraped 2840 .........\n",
      "......... Number of jobs scraped 2850 .........\n",
      "......... Number of jobs scraped 2860 .........\n",
      "......... Number of jobs scraped 2870 .........\n",
      "......... Number of jobs scraped 2880 .........\n",
      "......... Number of jobs scraped 2890 .........\n",
      "......... Number of jobs scraped 2900 .........\n",
      "......... Number of jobs scraped 2910 .........\n",
      "......... Number of jobs scraped 2920 .........\n",
      "......... Number of jobs scraped 2930 .........\n",
      "......... Number of jobs scraped 2940 .........\n",
      "......... Number of jobs scraped 2950 .........\n",
      "......... Number of jobs scraped 2960 .........\n",
      "......... Number of jobs scraped 2970 .........\n",
      "......... Number of jobs scraped 2980 .........\n",
      "......... Number of jobs scraped 2990 .........\n",
      "......... Number of jobs scraped 3000 .........\n",
      "......... Number of jobs scraped 3010 .........\n",
      "......... Number of jobs scraped 3020 .........\n",
      "......... Number of jobs scraped 3030 .........\n",
      "......... Number of jobs scraped 3040 .........\n",
      "......... Number of jobs scraped 3050 .........\n",
      "......... Number of jobs scraped 3060 .........\n",
      "......... Number of jobs scraped 3070 .........\n",
      "......... Number of jobs scraped 3080 .........\n",
      "......... Number of jobs scraped 3090 .........\n",
      "......... Number of jobs scraped 3100 .........\n",
      "......... Number of jobs scraped 3110 .........\n",
      "......... Number of jobs scraped 3120 .........\n",
      "......... Number of jobs scraped 3130 .........\n",
      "......... Number of jobs scraped 3140 .........\n",
      "......... Number of jobs scraped 3150 .........\n",
      "......... Number of jobs scraped 3160 .........\n",
      "......... Number of jobs scraped 3170 .........\n",
      "......... Number of jobs scraped 3180 .........\n",
      "......... Number of jobs scraped 3190 .........\n",
      "......... Number of jobs scraped 3200 .........\n",
      "......... Number of jobs scraped 3210 .........\n",
      "......... Number of jobs scraped 3220 .........\n",
      "......... Number of jobs scraped 3230 .........\n",
      "......... Number of jobs scraped 3240 .........\n",
      "......... Number of jobs scraped 3250 .........\n",
      "......... Number of jobs scraped 3260 .........\n",
      "......... Number of jobs scraped 3270 .........\n",
      "......... Number of jobs scraped 3280 .........\n",
      "......... Number of jobs scraped 3290 .........\n",
      "......... Number of jobs scraped 3300 .........\n",
      "......... Number of jobs scraped 3310 .........\n",
      "......... Number of jobs scraped 3320 .........\n",
      "......... Number of jobs scraped 3330 .........\n",
      "......... Number of jobs scraped 3340 .........\n",
      "......... Number of jobs scraped 3350 .........\n",
      "......... Number of jobs scraped 3360 .........\n",
      "......... Number of jobs scraped 3370 .........\n",
      "......... Number of jobs scraped 3380 .........\n",
      "......... Number of jobs scraped 3390 .........\n",
      "......... Number of jobs scraped 3400 .........\n",
      "......... Number of jobs scraped 3410 .........\n",
      "......... Number of jobs scraped 3420 .........\n",
      "......... Number of jobs scraped 3430 .........\n",
      "......... Number of jobs scraped 3440 .........\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......... Number of jobs scraped 3450 .........\n",
      "......... Number of jobs scraped 3460 .........\n",
      "......... Number of jobs scraped 3470 .........\n",
      "......... Number of jobs scraped 3480 .........\n",
      "......... Number of jobs scraped 3490 .........\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_titles</th>\n",
       "      <th>companies</th>\n",
       "      <th>locations</th>\n",
       "      <th>reviews</th>\n",
       "      <th>salaries</th>\n",
       "      <th>job_types</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nData Scientist Intern\\n</td>\n",
       "      <td>\\n\\nNIKE INC</td>\n",
       "      <td>Beaverton, OR</td>\n",
       "      <td>4325.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nData Science Intern\\n</td>\n",
       "      <td>\\n\\nExpedia</td>\n",
       "      <td>Bellevue, WA 98004 (Downtown area)</td>\n",
       "      <td>467.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nAnalyst, Reporting &amp; Analytics\\n</td>\n",
       "      <td>\\n\\nT-Mobile</td>\n",
       "      <td>Bellevue, WA 98006 (Somerset area)</td>\n",
       "      <td>6697.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nAnalytics Intern - Central Technology - Boul...</td>\n",
       "      <td>\\n\\nActivision</td>\n",
       "      <td>Boulder, CO</td>\n",
       "      <td>59.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nContract Support Assistant\\n</td>\n",
       "      <td>\\n\\nCubic Corporation</td>\n",
       "      <td>Remote</td>\n",
       "      <td>138.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\nData Analyst (Remote)\\n</td>\n",
       "      <td>\\nFirst San Francisco Partners</td>\n",
       "      <td>Remote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\nData Conversion Specialist\\n</td>\n",
       "      <td>\\n\\nPaylocity</td>\n",
       "      <td>Remote</td>\n",
       "      <td>49.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\nData Analytics Co-Op - Advanced Degrees\\n</td>\n",
       "      <td>\\n\\nAT&amp;T</td>\n",
       "      <td>California</td>\n",
       "      <td>19458.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\nData Scientist Intern\\n</td>\n",
       "      <td>\\n\\nIllumina</td>\n",
       "      <td>San Diego, CA 92122</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\nData Analyst (PCMH)\\n</td>\n",
       "      <td>\\n\\nProspect Medical Systems</td>\n",
       "      <td>Rhode Island</td>\n",
       "      <td>32.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\\nData Scientist Intern\\n</td>\n",
       "      <td>\\n\\nVolkswagen Group of America, Inc.</td>\n",
       "      <td>Auburn Hills, MI</td>\n",
       "      <td>3706.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\\nData Analytics Co-Op - Advanced Degrees\\n</td>\n",
       "      <td>\\n\\nAT&amp;T</td>\n",
       "      <td>California</td>\n",
       "      <td>19458.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>\\nData Scientist Intern\\n</td>\n",
       "      <td>\\n\\nAsurion</td>\n",
       "      <td>Nashville, TN 37214</td>\n",
       "      <td>1177.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\\nData Protection Officer\\n</td>\n",
       "      <td>\\n\\nGroupon</td>\n",
       "      <td>Chicago, IL 60654 (Loop area)</td>\n",
       "      <td>549.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\\nAssociate Data Scientist / Pricing Analyst\\n</td>\n",
       "      <td>\\n\\nExpedia</td>\n",
       "      <td>Bellevue, WA 98004 (Downtown area)</td>\n",
       "      <td>467.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\\nProduct Analyst, Data Science\\n</td>\n",
       "      <td>\\n\\nGoogle</td>\n",
       "      <td>Mountain View, CA</td>\n",
       "      <td>1787.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>\\nData Analyst (PCMH)\\n</td>\n",
       "      <td>\\n\\nProspect Medical Systems</td>\n",
       "      <td>Rhode Island</td>\n",
       "      <td>32.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>\\nData Analytics and Business Intelligence Ana...</td>\n",
       "      <td>\\n\\nApple</td>\n",
       "      <td>Sacramento, CA 95823 (Parkway area)</td>\n",
       "      <td>4210.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>\\nClinical Data Reviewer\\n</td>\n",
       "      <td>\\nKatalyst Healthcares &amp; Life Sciences</td>\n",
       "      <td>Remote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>\\nRESEARCH ASSISTANT\\n</td>\n",
       "      <td>\\n\\nMemorial Hospital</td>\n",
       "      <td>Rhode Island</td>\n",
       "      <td>684.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job_titles  \\\n",
       "0                           \\nData Scientist Intern\\n   \n",
       "1                             \\nData Science Intern\\n   \n",
       "2                  \\nAnalyst, Reporting & Analytics\\n   \n",
       "3   \\nAnalytics Intern - Central Technology - Boul...   \n",
       "4                      \\nContract Support Assistant\\n   \n",
       "5                           \\nData Analyst (Remote)\\n   \n",
       "6                      \\nData Conversion Specialist\\n   \n",
       "7         \\nData Analytics Co-Op - Advanced Degrees\\n   \n",
       "8                           \\nData Scientist Intern\\n   \n",
       "9                             \\nData Analyst (PCMH)\\n   \n",
       "10                          \\nData Scientist Intern\\n   \n",
       "11        \\nData Analytics Co-Op - Advanced Degrees\\n   \n",
       "12                          \\nData Scientist Intern\\n   \n",
       "13                        \\nData Protection Officer\\n   \n",
       "14     \\nAssociate Data Scientist / Pricing Analyst\\n   \n",
       "15                  \\nProduct Analyst, Data Science\\n   \n",
       "16                            \\nData Analyst (PCMH)\\n   \n",
       "17  \\nData Analytics and Business Intelligence Ana...   \n",
       "18                         \\nClinical Data Reviewer\\n   \n",
       "19                             \\nRESEARCH ASSISTANT\\n   \n",
       "\n",
       "                                 companies  \\\n",
       "0                             \\n\\nNIKE INC   \n",
       "1                              \\n\\nExpedia   \n",
       "2                             \\n\\nT-Mobile   \n",
       "3                           \\n\\nActivision   \n",
       "4                    \\n\\nCubic Corporation   \n",
       "5           \\nFirst San Francisco Partners   \n",
       "6                            \\n\\nPaylocity   \n",
       "7                                 \\n\\nAT&T   \n",
       "8                             \\n\\nIllumina   \n",
       "9             \\n\\nProspect Medical Systems   \n",
       "10   \\n\\nVolkswagen Group of America, Inc.   \n",
       "11                                \\n\\nAT&T   \n",
       "12                             \\n\\nAsurion   \n",
       "13                             \\n\\nGroupon   \n",
       "14                             \\n\\nExpedia   \n",
       "15                              \\n\\nGoogle   \n",
       "16            \\n\\nProspect Medical Systems   \n",
       "17                               \\n\\nApple   \n",
       "18  \\nKatalyst Healthcares & Life Sciences   \n",
       "19                   \\n\\nMemorial Hospital   \n",
       "\n",
       "                              locations  reviews salaries  job_types  ratings  \n",
       "0                         Beaverton, OR   4325.0      NaN        NaN     52.2  \n",
       "1    Bellevue, WA 98004 (Downtown area)    467.0      NaN        NaN     51.0  \n",
       "2    Bellevue, WA 98006 (Somerset area)   6697.0      NaN        NaN     51.0  \n",
       "3                           Boulder, CO     59.0      NaN        NaN     51.6  \n",
       "4                                Remote    138.0      NaN        NaN     42.6  \n",
       "5                                Remote      NaN      NaN        NaN      NaN  \n",
       "6                                Remote     49.0      NaN        NaN     41.4  \n",
       "7                            California  19458.0      NaN        NaN     44.4  \n",
       "8                   San Diego, CA 92122     99.0      NaN        NaN     42.6  \n",
       "9                          Rhode Island     32.0      NaN        NaN     31.8  \n",
       "10                     Auburn Hills, MI   3706.0      NaN        NaN     51.6  \n",
       "11                           California  19458.0      NaN        NaN     44.4  \n",
       "12                  Nashville, TN 37214   1177.0      NaN        NaN     43.8  \n",
       "13        Chicago, IL 60654 (Loop area)    549.0      NaN        NaN     42.6  \n",
       "14   Bellevue, WA 98004 (Downtown area)    467.0      NaN        NaN     51.0  \n",
       "15                    Mountain View, CA   1787.0      NaN        NaN     52.8  \n",
       "16                         Rhode Island     32.0      NaN        NaN     31.8  \n",
       "17  Sacramento, CA 95823 (Parkway area)   4210.0      NaN        NaN     52.2  \n",
       "18                               Remote      NaN      NaN        NaN      NaN  \n",
       "19                         Rhode Island    684.0      NaN        NaN     44.4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find max pages\n",
    "\n",
    "url = 'https://www.indeed.com/jobs?q=data&start='\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "max_soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "max_page = int(max_soup.find('div',{'id':'searchCount'}).text.split(' ')[-1].replace(',',''))\n",
    "\n",
    "if max_page % 10 == 0:\n",
    "    max_page = max_page/10\n",
    "else:\n",
    "    max_page = max_page/10 + 1\n",
    "    \n",
    "\n",
    "job_titles_ = []\n",
    "companies_ = []\n",
    "locations_ = []\n",
    "reviews_ = []\n",
    "salaries_ = []\n",
    "job_types_ = []\n",
    "ratings_ = []\n",
    "\n",
    "for i in range(0,3490,10):\n",
    "    \n",
    "    url = url+str(i)\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'lxml').find_all('div', {\"data-tn-component\":\"organicJob\"})\n",
    "    \n",
    "    for chunk in soup:\n",
    "    \n",
    "        job_titles = job_titles_.extend(scrape_title(chunk))\n",
    "        companies = companies_.extend(scrape_company(chunk))\n",
    "        locations = locations_.extend(scrape_location(chunk))\n",
    "        reviews = reviews_.extend(scrape_review(chunk))\n",
    "        salaries = salaries_.extend(scrape_salary(chunk))\n",
    "        job_types = job_types_.extend(scrape_jobtype(chunk))\n",
    "        ratings = ratings_.extend(scrape_rating(chunk))\n",
    "\n",
    "    print '......... Number of jobs scraped ' + str(i + 10)+ ' .........'\n",
    "\n",
    "    time.sleep(3)\n",
    "\n",
    "results = pd.DataFrame()\n",
    "\n",
    "results['job_titles'] = job_titles_\n",
    "results['companies'] = companies_\n",
    "results['locations'] = locations_\n",
    "results['reviews'] = reviews_\n",
    "results['salaries'] = salaries_\n",
    "results['job_types'] = job_types_\n",
    "results['ratings'] = ratings_\n",
    "\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_pickle('results.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......... Number of jobs scraped 3510 .........\n",
      "......... Number of jobs scraped 3520 .........\n",
      "......... Number of jobs scraped 3530 .........\n",
      "......... Number of jobs scraped 3540 .........\n",
      "......... Number of jobs scraped 3550 .........\n",
      "......... Number of jobs scraped 3560 .........\n",
      "......... Number of jobs scraped 3570 .........\n",
      "......... Number of jobs scraped 3580 .........\n",
      "......... Number of jobs scraped 3590 .........\n",
      "......... Number of jobs scraped 3600 .........\n",
      "......... Number of jobs scraped 3610 .........\n",
      "......... Number of jobs scraped 3620 .........\n",
      "......... Number of jobs scraped 3630 .........\n",
      "......... Number of jobs scraped 3640 .........\n",
      "......... Number of jobs scraped 3650 .........\n",
      "......... Number of jobs scraped 3660 .........\n",
      "......... Number of jobs scraped 3670 .........\n",
      "......... Number of jobs scraped 3680 .........\n",
      "......... Number of jobs scraped 3690 .........\n",
      "......... Number of jobs scraped 3700 .........\n",
      "......... Number of jobs scraped 3710 .........\n",
      "......... Number of jobs scraped 3720 .........\n",
      "......... Number of jobs scraped 3730 .........\n",
      "......... Number of jobs scraped 3740 .........\n",
      "......... Number of jobs scraped 3750 .........\n",
      "......... Number of jobs scraped 3760 .........\n",
      "......... Number of jobs scraped 3770 .........\n",
      "......... Number of jobs scraped 3780 .........\n",
      "......... Number of jobs scraped 3790 .........\n",
      "......... Number of jobs scraped 3800 .........\n",
      "......... Number of jobs scraped 3810 .........\n",
      "......... Number of jobs scraped 3820 .........\n",
      "......... Number of jobs scraped 3830 .........\n",
      "......... Number of jobs scraped 3840 .........\n",
      "......... Number of jobs scraped 3850 .........\n",
      "......... Number of jobs scraped 3860 .........\n",
      "......... Number of jobs scraped 3870 .........\n",
      "......... Number of jobs scraped 3880 .........\n",
      "......... Number of jobs scraped 3890 .........\n",
      "......... Number of jobs scraped 3900 .........\n",
      "......... Number of jobs scraped 3910 .........\n",
      "......... Number of jobs scraped 3920 .........\n",
      "......... Number of jobs scraped 3930 .........\n",
      "......... Number of jobs scraped 3940 .........\n",
      "......... Number of jobs scraped 3950 .........\n",
      "......... Number of jobs scraped 3960 .........\n",
      "......... Number of jobs scraped 3970 .........\n",
      "......... Number of jobs scraped 3980 .........\n",
      "......... Number of jobs scraped 3990 .........\n",
      "......... Number of jobs scraped 4000 .........\n",
      "......... Number of jobs scraped 4010 .........\n",
      "......... Number of jobs scraped 4020 .........\n",
      "......... Number of jobs scraped 4030 .........\n",
      "......... Number of jobs scraped 4040 .........\n",
      "......... Number of jobs scraped 4050 .........\n",
      "......... Number of jobs scraped 4060 .........\n",
      "......... Number of jobs scraped 4070 .........\n",
      "......... Number of jobs scraped 4080 .........\n",
      "......... Number of jobs scraped 4090 .........\n",
      "......... Number of jobs scraped 4100 .........\n",
      "......... Number of jobs scraped 4110 .........\n",
      "......... Number of jobs scraped 4120 .........\n",
      "......... Number of jobs scraped 4130 .........\n",
      "......... Number of jobs scraped 4140 .........\n",
      "......... Number of jobs scraped 4150 .........\n",
      "......... Number of jobs scraped 4160 .........\n",
      "......... Number of jobs scraped 4170 .........\n",
      "......... Number of jobs scraped 4180 .........\n",
      "......... Number of jobs scraped 4190 .........\n",
      "......... Number of jobs scraped 4200 .........\n",
      "......... Number of jobs scraped 4210 .........\n",
      "......... Number of jobs scraped 4220 .........\n",
      "......... Number of jobs scraped 4230 .........\n",
      "......... Number of jobs scraped 4240 .........\n",
      "......... Number of jobs scraped 4250 .........\n",
      "......... Number of jobs scraped 4260 .........\n",
      "......... Number of jobs scraped 4270 .........\n",
      "......... Number of jobs scraped 4280 .........\n",
      "......... Number of jobs scraped 4290 .........\n",
      "......... Number of jobs scraped 4300 .........\n",
      "......... Number of jobs scraped 4310 .........\n",
      "......... Number of jobs scraped 4320 .........\n",
      "......... Number of jobs scraped 4330 .........\n",
      "......... Number of jobs scraped 4340 .........\n",
      "......... Number of jobs scraped 4350 .........\n",
      "......... Number of jobs scraped 4360 .........\n",
      "......... Number of jobs scraped 4370 .........\n",
      "......... Number of jobs scraped 4380 .........\n",
      "......... Number of jobs scraped 4390 .........\n",
      "......... Number of jobs scraped 4400 .........\n",
      "......... Number of jobs scraped 4410 .........\n",
      "......... Number of jobs scraped 4420 .........\n",
      "......... Number of jobs scraped 4430 .........\n",
      "......... Number of jobs scraped 4440 .........\n",
      "......... Number of jobs scraped 4450 .........\n",
      "......... Number of jobs scraped 4460 .........\n",
      "......... Number of jobs scraped 4470 .........\n",
      "......... Number of jobs scraped 4480 .........\n",
      "......... Number of jobs scraped 4490 .........\n",
      "......... Number of jobs scraped 4500 .........\n",
      "......... Number of jobs scraped 4510 .........\n",
      "......... Number of jobs scraped 4520 .........\n",
      "......... Number of jobs scraped 4530 .........\n",
      "......... Number of jobs scraped 4540 .........\n",
      "......... Number of jobs scraped 4550 .........\n",
      "......... Number of jobs scraped 4560 .........\n",
      "......... Number of jobs scraped 4570 .........\n",
      "......... Number of jobs scraped 4580 .........\n",
      "......... Number of jobs scraped 4590 .........\n",
      "......... Number of jobs scraped 4600 .........\n",
      "......... Number of jobs scraped 4610 .........\n",
      "......... Number of jobs scraped 4620 .........\n",
      "......... Number of jobs scraped 4630 .........\n",
      "......... Number of jobs scraped 4640 .........\n",
      "......... Number of jobs scraped 4650 .........\n",
      "......... Number of jobs scraped 4660 .........\n",
      "......... Number of jobs scraped 4670 .........\n",
      "......... Number of jobs scraped 4680 .........\n",
      "......... Number of jobs scraped 4690 .........\n",
      "......... Number of jobs scraped 4700 .........\n",
      "......... Number of jobs scraped 4710 .........\n",
      "......... Number of jobs scraped 4720 .........\n",
      "......... Number of jobs scraped 4730 .........\n",
      "......... Number of jobs scraped 4740 .........\n",
      "......... Number of jobs scraped 4750 .........\n",
      "......... Number of jobs scraped 4760 .........\n",
      "......... Number of jobs scraped 4770 .........\n",
      "......... Number of jobs scraped 4780 .........\n",
      "......... Number of jobs scraped 4790 .........\n",
      "......... Number of jobs scraped 4800 .........\n",
      "......... Number of jobs scraped 4810 .........\n",
      "......... Number of jobs scraped 4820 .........\n",
      "......... Number of jobs scraped 4830 .........\n",
      "......... Number of jobs scraped 4840 .........\n",
      "......... Number of jobs scraped 4850 .........\n",
      "......... Number of jobs scraped 4860 .........\n",
      "......... Number of jobs scraped 4870 .........\n",
      "......... Number of jobs scraped 4880 .........\n",
      "......... Number of jobs scraped 4890 .........\n",
      "......... Number of jobs scraped 4900 .........\n",
      "......... Number of jobs scraped 4910 .........\n",
      "......... Number of jobs scraped 4920 .........\n",
      "......... Number of jobs scraped 4930 .........\n",
      "......... Number of jobs scraped 4940 .........\n",
      "......... Number of jobs scraped 4950 .........\n",
      "......... Number of jobs scraped 4960 .........\n",
      "......... Number of jobs scraped 4970 .........\n",
      "......... Number of jobs scraped 4980 .........\n",
      "......... Number of jobs scraped 4990 .........\n",
      "......... Number of jobs scraped 5000 .........\n",
      "......... Number of jobs scraped 5010 .........\n",
      "......... Number of jobs scraped 5020 .........\n",
      "......... Number of jobs scraped 5030 .........\n",
      "......... Number of jobs scraped 5040 .........\n",
      "......... Number of jobs scraped 5050 .........\n",
      "......... Number of jobs scraped 5060 .........\n",
      "......... Number of jobs scraped 5070 .........\n",
      "......... Number of jobs scraped 5080 .........\n",
      "......... Number of jobs scraped 5090 .........\n",
      "......... Number of jobs scraped 5100 .........\n",
      "......... Number of jobs scraped 5110 .........\n",
      "......... Number of jobs scraped 5120 .........\n",
      "......... Number of jobs scraped 5130 .........\n",
      "......... Number of jobs scraped 5140 .........\n",
      "......... Number of jobs scraped 5150 .........\n",
      "......... Number of jobs scraped 5160 .........\n",
      "......... Number of jobs scraped 5170 .........\n",
      "......... Number of jobs scraped 5180 .........\n",
      "......... Number of jobs scraped 5190 .........\n",
      "......... Number of jobs scraped 5200 .........\n",
      "......... Number of jobs scraped 5210 .........\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......... Number of jobs scraped 5220 .........\n",
      "......... Number of jobs scraped 5230 .........\n",
      "......... Number of jobs scraped 5240 .........\n",
      "......... Number of jobs scraped 5250 .........\n",
      "......... Number of jobs scraped 5260 .........\n",
      "......... Number of jobs scraped 5270 .........\n",
      "......... Number of jobs scraped 5280 .........\n",
      "......... Number of jobs scraped 5290 .........\n",
      "......... Number of jobs scraped 5300 .........\n",
      "......... Number of jobs scraped 5310 .........\n",
      "......... Number of jobs scraped 5320 .........\n",
      "......... Number of jobs scraped 5330 .........\n",
      "......... Number of jobs scraped 5340 .........\n",
      "......... Number of jobs scraped 5350 .........\n",
      "......... Number of jobs scraped 5360 .........\n",
      "......... Number of jobs scraped 5370 .........\n",
      "......... Number of jobs scraped 5380 .........\n",
      "......... Number of jobs scraped 5390 .........\n",
      "......... Number of jobs scraped 5400 .........\n",
      "......... Number of jobs scraped 5410 .........\n",
      "......... Number of jobs scraped 5420 .........\n",
      "......... Number of jobs scraped 5430 .........\n",
      "......... Number of jobs scraped 5440 .........\n",
      "......... Number of jobs scraped 5450 .........\n",
      "......... Number of jobs scraped 5460 .........\n",
      "......... Number of jobs scraped 5470 .........\n",
      "......... Number of jobs scraped 5480 .........\n",
      "......... Number of jobs scraped 5490 .........\n",
      "......... Number of jobs scraped 5500 .........\n",
      "......... Number of jobs scraped 5510 .........\n",
      "......... Number of jobs scraped 5520 .........\n",
      "......... Number of jobs scraped 5530 .........\n",
      "......... Number of jobs scraped 5540 .........\n",
      "......... Number of jobs scraped 5550 .........\n",
      "......... Number of jobs scraped 5560 .........\n",
      "......... Number of jobs scraped 5570 .........\n",
      "......... Number of jobs scraped 5580 .........\n",
      "......... Number of jobs scraped 5590 .........\n",
      "......... Number of jobs scraped 5600 .........\n",
      "......... Number of jobs scraped 5610 .........\n",
      "......... Number of jobs scraped 5620 .........\n",
      "......... Number of jobs scraped 5630 .........\n",
      "......... Number of jobs scraped 5640 .........\n",
      "......... Number of jobs scraped 5650 .........\n",
      "......... Number of jobs scraped 5660 .........\n",
      "......... Number of jobs scraped 5670 .........\n",
      "......... Number of jobs scraped 5680 .........\n",
      "......... Number of jobs scraped 5690 .........\n",
      "......... Number of jobs scraped 5700 .........\n",
      "......... Number of jobs scraped 5710 .........\n",
      "......... Number of jobs scraped 5720 .........\n",
      "......... Number of jobs scraped 5730 .........\n",
      "......... Number of jobs scraped 5740 .........\n",
      "......... Number of jobs scraped 5750 .........\n",
      "......... Number of jobs scraped 5760 .........\n",
      "......... Number of jobs scraped 5770 .........\n",
      "......... Number of jobs scraped 5780 .........\n",
      "......... Number of jobs scraped 5790 .........\n",
      "......... Number of jobs scraped 5800 .........\n",
      "......... Number of jobs scraped 5810 .........\n",
      "......... Number of jobs scraped 5820 .........\n",
      "......... Number of jobs scraped 5830 .........\n",
      "......... Number of jobs scraped 5840 .........\n",
      "......... Number of jobs scraped 5850 .........\n",
      "......... Number of jobs scraped 5860 .........\n",
      "......... Number of jobs scraped 5870 .........\n",
      "......... Number of jobs scraped 5880 .........\n",
      "......... Number of jobs scraped 5890 .........\n",
      "......... Number of jobs scraped 5900 .........\n",
      "......... Number of jobs scraped 5910 .........\n",
      "......... Number of jobs scraped 5920 .........\n",
      "......... Number of jobs scraped 5930 .........\n",
      "......... Number of jobs scraped 5940 .........\n",
      "......... Number of jobs scraped 5950 .........\n",
      "......... Number of jobs scraped 5960 .........\n",
      "......... Number of jobs scraped 5970 .........\n",
      "......... Number of jobs scraped 5980 .........\n",
      "......... Number of jobs scraped 5990 .........\n",
      "......... Number of jobs scraped 6000 .........\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_titles</th>\n",
       "      <th>companies</th>\n",
       "      <th>locations</th>\n",
       "      <th>reviews</th>\n",
       "      <th>salaries</th>\n",
       "      <th>job_types</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nMCM Analytics and Reporting\\n</td>\n",
       "      <td>\\n\\nQuintilesIMS</td>\n",
       "      <td>Plymouth Meeting, PA</td>\n",
       "      <td>913.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nMES Application Analyst\\n</td>\n",
       "      <td>\\n\\nFinisar</td>\n",
       "      <td>Fremont, CA 94538 (Irvington area)</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nJunior Analyst\\n</td>\n",
       "      <td>\\n\\nMorgan Borszcz Consulting</td>\n",
       "      <td>Cherry Point, NC</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nData Analyst\\n</td>\n",
       "      <td>\\n\\nPlanSource</td>\n",
       "      <td>Orlando, FL</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nJr. Data Analyst and Programmer (Health Cons...</td>\n",
       "      <td>\\n\\nLogistics Management Institute</td>\n",
       "      <td>Tysons, VA</td>\n",
       "      <td>22.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\nData Analyst\\n</td>\n",
       "      <td>\\n\\nGroup O</td>\n",
       "      <td>San Antonio, TX</td>\n",
       "      <td>85.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\nAssociate Data Scientist\\n</td>\n",
       "      <td>\\n\\nLimeade</td>\n",
       "      <td>Bellevue, WA 98004 (Downtown area)</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\nData Processing Operator - Optics\\n</td>\n",
       "      <td>\\n\\nHarris Corporation</td>\n",
       "      <td>Kekaha, HI 96752</td>\n",
       "      <td>353.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\nData Administrator\\n</td>\n",
       "      <td>\\n\\nNextgen Technologies</td>\n",
       "      <td>United States</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\nBusiness Analyst ? Capital Markets Analytics...</td>\n",
       "      <td>\\n\\nFirst Guaranty Mortgage Corporation</td>\n",
       "      <td>Plano, TX 75023</td>\n",
       "      <td>22.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\\nMCM Analytics and Reporting\\n</td>\n",
       "      <td>\\n\\nQuintilesIMS</td>\n",
       "      <td>Plymouth Meeting, PA</td>\n",
       "      <td>913.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\\nMES Application Analyst\\n</td>\n",
       "      <td>\\n\\nFinisar</td>\n",
       "      <td>Fremont, CA 94538 (Irvington area)</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>\\nJunior Analyst\\n</td>\n",
       "      <td>\\n\\nMorgan Borszcz Consulting</td>\n",
       "      <td>Cherry Point, NC</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\\nData Analyst\\n</td>\n",
       "      <td>\\n\\nPlanSource</td>\n",
       "      <td>Orlando, FL</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\\nJr. Data Analyst and Programmer (Health Cons...</td>\n",
       "      <td>\\n\\nLogistics Management Institute</td>\n",
       "      <td>Tysons, VA</td>\n",
       "      <td>22.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\\nData Analyst\\n</td>\n",
       "      <td>\\n\\nGroup O</td>\n",
       "      <td>San Antonio, TX</td>\n",
       "      <td>85.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>\\nAssociate Data Scientist\\n</td>\n",
       "      <td>\\n\\nLimeade</td>\n",
       "      <td>Bellevue, WA 98004 (Downtown area)</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>\\nData Processing Operator - Optics\\n</td>\n",
       "      <td>\\n\\nHarris Corporation</td>\n",
       "      <td>Kekaha, HI 96752</td>\n",
       "      <td>353.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>\\nData Administrator\\n</td>\n",
       "      <td>\\n\\nNextgen Technologies</td>\n",
       "      <td>United States</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>\\nBusiness Analyst ? Capital Markets Analytics...</td>\n",
       "      <td>\\n\\nFirst Guaranty Mortgage Corporation</td>\n",
       "      <td>Plano, TX 75023</td>\n",
       "      <td>22.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job_titles  \\\n",
       "0                     \\nMCM Analytics and Reporting\\n   \n",
       "1                         \\nMES Application Analyst\\n   \n",
       "2                                  \\nJunior Analyst\\n   \n",
       "3                                    \\nData Analyst\\n   \n",
       "4   \\nJr. Data Analyst and Programmer (Health Cons...   \n",
       "5                                    \\nData Analyst\\n   \n",
       "6                        \\nAssociate Data Scientist\\n   \n",
       "7               \\nData Processing Operator - Optics\\n   \n",
       "8                              \\nData Administrator\\n   \n",
       "9   \\nBusiness Analyst ? Capital Markets Analytics...   \n",
       "10                    \\nMCM Analytics and Reporting\\n   \n",
       "11                        \\nMES Application Analyst\\n   \n",
       "12                                 \\nJunior Analyst\\n   \n",
       "13                                   \\nData Analyst\\n   \n",
       "14  \\nJr. Data Analyst and Programmer (Health Cons...   \n",
       "15                                   \\nData Analyst\\n   \n",
       "16                       \\nAssociate Data Scientist\\n   \n",
       "17              \\nData Processing Operator - Optics\\n   \n",
       "18                             \\nData Administrator\\n   \n",
       "19  \\nBusiness Analyst ? Capital Markets Analytics...   \n",
       "\n",
       "                                  companies  \\\n",
       "0                          \\n\\nQuintilesIMS   \n",
       "1                               \\n\\nFinisar   \n",
       "2             \\n\\nMorgan Borszcz Consulting   \n",
       "3                            \\n\\nPlanSource   \n",
       "4        \\n\\nLogistics Management Institute   \n",
       "5                               \\n\\nGroup O   \n",
       "6                               \\n\\nLimeade   \n",
       "7                    \\n\\nHarris Corporation   \n",
       "8                  \\n\\nNextgen Technologies   \n",
       "9   \\n\\nFirst Guaranty Mortgage Corporation   \n",
       "10                         \\n\\nQuintilesIMS   \n",
       "11                              \\n\\nFinisar   \n",
       "12            \\n\\nMorgan Borszcz Consulting   \n",
       "13                           \\n\\nPlanSource   \n",
       "14       \\n\\nLogistics Management Institute   \n",
       "15                              \\n\\nGroup O   \n",
       "16                              \\n\\nLimeade   \n",
       "17                   \\n\\nHarris Corporation   \n",
       "18                 \\n\\nNextgen Technologies   \n",
       "19  \\n\\nFirst Guaranty Mortgage Corporation   \n",
       "\n",
       "                             locations  reviews salaries  job_types  ratings  \n",
       "0                 Plymouth Meeting, PA    913.0      NaN        NaN     44.4  \n",
       "1   Fremont, CA 94538 (Irvington area)     47.0      NaN        NaN     43.2  \n",
       "2                     Cherry Point, NC     11.0      NaN        NaN     51.0  \n",
       "3                          Orlando, FL     23.0      NaN        NaN     41.4  \n",
       "4                           Tysons, VA     22.0      NaN        NaN     51.0  \n",
       "5                      San Antonio, TX     85.0      NaN        NaN     40.2  \n",
       "6   Bellevue, WA 98004 (Downtown area)      6.0      NaN        NaN     30.0  \n",
       "7                     Kekaha, HI 96752    353.0      NaN        NaN     42.6  \n",
       "8                        United States      2.0      NaN        NaN     39.0  \n",
       "9                      Plano, TX 75023     22.0      NaN        NaN     32.4  \n",
       "10                Plymouth Meeting, PA    913.0      NaN        NaN     44.4  \n",
       "11  Fremont, CA 94538 (Irvington area)     47.0      NaN        NaN     43.2  \n",
       "12                    Cherry Point, NC     11.0      NaN        NaN     51.0  \n",
       "13                         Orlando, FL     23.0      NaN        NaN     41.4  \n",
       "14                          Tysons, VA     22.0      NaN        NaN     51.0  \n",
       "15                     San Antonio, TX     85.0      NaN        NaN     40.2  \n",
       "16  Bellevue, WA 98004 (Downtown area)      6.0      NaN        NaN     30.0  \n",
       "17                    Kekaha, HI 96752    353.0      NaN        NaN     42.6  \n",
       "18                       United States      2.0      NaN        NaN     39.0  \n",
       "19                     Plano, TX 75023     22.0      NaN        NaN     32.4  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find max pages\n",
    "\n",
    "url = 'https://www.indeed.com/jobs?q=data&start='\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "max_soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "#max_page = int(max_soup.find('div',{'id':'searchCount'}).text.split(' ')[-1].replace(',',''))\n",
    "\n",
    "#if max_page % 10 == 0:\n",
    "#    max_page = max_page/10\n",
    "#else:\n",
    "#    max_page = max_page/10 + 1\n",
    "    \n",
    "\n",
    "job_titles_ = []\n",
    "companies_ = []\n",
    "locations_ = []\n",
    "reviews_ = []\n",
    "salaries_ = []\n",
    "job_types_ = []\n",
    "ratings_ = []\n",
    "\n",
    "for i in range(3500,6000,10):\n",
    "    \n",
    "    url = url+str(i)\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'lxml').find_all('div', {\"data-tn-component\":\"organicJob\"})\n",
    "    \n",
    "    for chunk in soup:\n",
    "    \n",
    "        job_titles = job_titles_.extend(scrape_title(chunk))\n",
    "        companies = companies_.extend(scrape_company(chunk))\n",
    "        locations = locations_.extend(scrape_location(chunk))\n",
    "        reviews = reviews_.extend(scrape_review(chunk))\n",
    "        salaries = salaries_.extend(scrape_salary(chunk))\n",
    "        job_types = job_types_.extend(scrape_jobtype(chunk))\n",
    "        ratings = ratings_.extend(scrape_rating(chunk))\n",
    "\n",
    "    print '......... Number of jobs scraped ' + str(i + 10)+ ' .........'\n",
    "\n",
    "    time.sleep(3)\n",
    "\n",
    "results = pd.DataFrame()\n",
    "\n",
    "results['job_titles'] = job_titles_\n",
    "results['companies'] = companies_\n",
    "results['locations'] = locations_\n",
    "results['reviews'] = reviews_\n",
    "results['salaries'] = salaries_\n",
    "results['job_types'] = job_types_\n",
    "results['ratings'] = ratings_\n",
    "\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results.to_pickle('results_2.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2496"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.salaries.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2500 entries, 0 to 2499\n",
      "Data columns (total 7 columns):\n",
      "job_titles    2500 non-null object\n",
      "companies     2500 non-null object\n",
      "locations     2500 non-null object\n",
      "reviews       2247 non-null float64\n",
      "salaries      4 non-null object\n",
      "job_types     0 non-null float64\n",
      "ratings       2247 non-null float64\n",
      "dtypes: float64(3), object(4)\n",
      "memory usage: 136.8+ KB\n"
     ]
    }
   ],
   "source": [
    "results.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
