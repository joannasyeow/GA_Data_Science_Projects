{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:900px;background:#D9D9D9;border:1px solid black;text-align:left;padding:8px;\">\n",
    "\n",
    "\n",
    "\n",
    "<p>\n",
    "<b>Step 1:</b> URL = https://www.indeed.com\n",
    "<br><b>Step 2:</b> Keyword search parameter - \"Data\"\n",
    "<br><b>Step 3:</b> Location choice - USA (more data is avaliable)\n",
    "<br><b>Step 4:</b> Loop through 6 salary search parameter - \"25000\",\"35000\",\"50000\",\"65000\",\"80000\"\n",
    "<br><b>Step 5:</b> Each salary parameter scrape 100 pages (1000 jobs)\n",
    "<br><b>Step 6:</b> Scrape information: <b>job title, location, salary, ratings, reviews, summary</b>\n",
    "<br><b>Step 7:</b> Save data into pickle file\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:900px;background:#D9D9D9;border:1px solid black;text-align:left;padding:8px;\">\n",
    "\n",
    "\n",
    "\n",
    "<p>\n",
    "<b>Job title, Companies, Summaries, Locations</b>\n",
    "<br>1. Change data type from unicode to string. \n",
    "<br>2. Remove punctuations and special characters and lower case all text (except location)\n",
    "<br>\n",
    "<br><b> Location </b> \n",
    "<br>1. Replace with state abbrevations (e.g. AL, SF)\n",
    "<br>\n",
    "<br><b>Reviews, Ratings</b>\n",
    "<br>1. Fill NaN with 0.0\n",
    "<br><br><b>Salaries</b>\n",
    "<br>1. If information is avaiable, scale into annual salary. \n",
    "<br>2. If salary range is provided, obtain mean of range. \n",
    "<br>3. If information not avaliable, impute with estimated salary (what was input in search bar during scrapping) \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def salary_func(x):\n",
    "    \n",
    "    \"\"\"Extracts salary information if avaliable - and scale them into annual salaries\n",
    "    if salary range is provided, take mean of range\"\"\"\n",
    "\n",
    "    try:\n",
    "        salary = np.mean([float(s) for s in re.findall(r'\\d+',a.replace(',','').replace('$',''))])\n",
    "\n",
    "        if 'hour' in x:\n",
    "            salary = salary * 2080\n",
    "        \n",
    "        elif 'day' in x:\n",
    "            salary = salary * 260\n",
    "            \n",
    "        elif 'week' in x:\n",
    "            salary = salary * 52\n",
    "        \n",
    "        \n",
    "    except:\n",
    "        salary = np.nan\n",
    "        \n",
    "    return salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobs_file = pd.DataFrame()\n",
    "\n",
    "for ind, filename in enumerate(glob.iglob('./*.pickle')):   # Iterates through all pickle file in folder\n",
    "    \n",
    "    # Open each file as data\n",
    "    with open(filename) as inputfile:\n",
    "        \n",
    "        data = pickle.load(inputfile)  # Load pickle file\n",
    "\n",
    "        # Clean text features\n",
    "        \n",
    "        data.job_titles = data.job_titles.map(lambda x : x.encode('ascii','ignore')\\\n",
    "                                              .replace('\\n','')\\\n",
    "                                              .translate(None, string.punctuation)\\\n",
    "                                              .lower())\n",
    "        \n",
    "        data.companies = data.companies.map(lambda x : x.encode('ascii','ignore')\\\n",
    "                                              .replace('\\n','')\\\n",
    "                                              .translate(None, string.punctuation)\\\n",
    "                                              .lower())\n",
    "        \n",
    "        data.locations = data.locations.map(lambda x : x.encode('ascii','ignore')\\\n",
    "                                              .replace('\\n','')\\\n",
    "                                              .translate(None, string.punctuation))\n",
    "        \n",
    "        data.summaries = data.summaries.map(lambda x : x.encode('ascii','ignore')\\\n",
    "                                              .replace('\\n','')\\\n",
    "                                              .translate(None, string.punctuation)\n",
    "                                              .lower())\n",
    "\n",
    "        # Impute NaN values with 0.0\n",
    "        \n",
    "        data.reviews = data.reviews.fillna(0.0)\n",
    "\n",
    "        data.ratings = data.ratings.fillna(0.0)\n",
    "        \n",
    "        # Extract salaries\n",
    "\n",
    "        data.salaries = data.salaries.map(salary_func)\n",
    "\n",
    "        # If exact salary is not provided, impute with salary that was searched on\n",
    "        \n",
    "        base_salary = [25000,35000,50000,65000,80000,100000]\n",
    "\n",
    "        for indx, base in enumerate(base_salary):\n",
    "\n",
    "            if ind == indx:\n",
    "                \n",
    "                data.salaries = data.salaries.fillna(base)\n",
    "\n",
    "        \n",
    "    jobs_file = jobs_file.append(data)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:900px;background:#D9D9D9;border:1px solid black;text-align:left;padding:8px;\">\n",
    "\n",
    "<p>\n",
    "Check for NaN values\n",
    "<br>\n",
    "Investigate outliers\n",
    "<br>\n",
    "Visualise correlation matrix and distributions\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8960 entries, 0 to 1499\n",
      "Data columns (total 7 columns):\n",
      "job_titles    8960 non-null object\n",
      "companies     8960 non-null object\n",
      "locations     8960 non-null object\n",
      "reviews       8960 non-null float64\n",
      "salaries      8960 non-null float64\n",
      "ratings       8960 non-null float64\n",
      "summaries     8960 non-null object\n",
      "dtypes: float64(3), object(4)\n",
      "memory usage: 560.0+ KB\n"
     ]
    }
   ],
   "source": [
    "jobs_file.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_titles</th>\n",
       "      <th>companies</th>\n",
       "      <th>locations</th>\n",
       "      <th>reviews</th>\n",
       "      <th>salaries</th>\n",
       "      <th>ratings</th>\n",
       "      <th>summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vice president  data integration manager  firm...</td>\n",
       "      <td>jp morgan chase</td>\n",
       "      <td>Jersey City NJ 07310 Downtown area</td>\n",
       "      <td>14863.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>44.4</td>\n",
       "      <td>the organizations objectives are to explain th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jr research analyst</td>\n",
       "      <td>wells fargo</td>\n",
       "      <td>San Francisco CA</td>\n",
       "      <td>19547.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>43.8</td>\n",
       "      <td>identifies potential data sources for statisti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data specialist solutions</td>\n",
       "      <td>facebook</td>\n",
       "      <td>Austin TX</td>\n",
       "      <td>203.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>52.2</td>\n",
       "      <td>effectively plan and collaborate with team mem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>market data analyst manager vice president</td>\n",
       "      <td>state street</td>\n",
       "      <td>New York NY</td>\n",
       "      <td>1356.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>o automate ec risk data enrichment and data go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>product data analyst  personalization</td>\n",
       "      <td>hulu</td>\n",
       "      <td>Seattle WA</td>\n",
       "      <td>28.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>44.4</td>\n",
       "      <td>passion for turning data into insights and hel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          job_titles        companies  \\\n",
       "0  vice president  data integration manager  firm...  jp morgan chase   \n",
       "1                                jr research analyst      wells fargo   \n",
       "2                          data specialist solutions         facebook   \n",
       "3         market data analyst manager vice president     state street   \n",
       "4              product data analyst  personalization             hulu   \n",
       "\n",
       "                            locations  reviews  salaries  ratings  \\\n",
       "0  Jersey City NJ 07310 Downtown area  14863.0   25000.0     44.4   \n",
       "1                    San Francisco CA  19547.0   25000.0     43.8   \n",
       "2                           Austin TX    203.0   25000.0     52.2   \n",
       "3                         New York NY   1356.0   25000.0     42.0   \n",
       "4                          Seattle WA     28.0   25000.0     44.4   \n",
       "\n",
       "                                           summaries  \n",
       "0  the organizations objectives are to explain th...  \n",
       "1  identifies potential data sources for statisti...  \n",
       "2  effectively plan and collaborate with team mem...  \n",
       "3  o automate ec risk data enrichment and data go...  \n",
       "4  passion for turning data into insights and hel...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning (Locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:900px;background:#D9D9D9;border:1px solid black;text-align:left;padding:8px;\">\n",
    "\n",
    "\n",
    "\n",
    "<p>\n",
    "Locations are in different format, I will be standardising them with abbrevations\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load state abbrevations data\n",
    "\n",
    "states_ = pd.read_csv('./us_states.csv').iloc[:,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_['Capital'] = states_['Capital'].str.lower()\n",
    "states_['State Name'] = states_['State Name'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_dict = states_.set_index('Abbreviation').T.to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []\n",
    "\n",
    "for state in jobs_file.locations:\n",
    "    \n",
    "    if state == 'Remote':\n",
    "        states.append('Remote')\n",
    "        \n",
    "    else:\n",
    "        abv = re.search(r'([A-Q][A-Q])',state)\n",
    "        \n",
    "        if abv is None:\n",
    "            \n",
    "            abb = 0\n",
    "            for k,v in states_dict.items():\n",
    "                if state.lower() in v:\n",
    "                    abb = k\n",
    "                else:\n",
    "                    abb = 'US'\n",
    "                \n",
    "            states.append(abb)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            states.append(abv.group())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_file.locations = states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:900px;background:#D9D9D9;border:1px solid black;text-align:left;padding:8px;\">\n",
    "\n",
    "\n",
    "\n",
    "<p>\n",
    "Based on the companies, I will engineer a new binary column on whether the company is in the Forbes US 500 list (2016 & 2017)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fortune16 = pd.read_csv('./fortune500_16.csv')['Company Name']     # Load data\n",
    "fortune17 = pd.read_csv('./fortune500_17.csv')['Company Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fortune = pd.concat([fortune16,fortune17]).drop_duplicates().reset_index(drop=True).dropna()  # Clean data\n",
    "fortune = fortune.str.translate(None, string.punctuation).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fortune500 = [1 if company in list(fortune) else 0 for company in jobs_file.companies] # 1 = company in Fortune500 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_file['fortune500'] = fortune500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:900px;background:#D9D9D9;border:1px solid black;text-align:left;padding:8px;\">\n",
    "\n",
    "\n",
    "\n",
    "<p>\n",
    "Scale numerical data - <b>Reviews, Ratings</b>\n",
    "<Br>\n",
    "Get dummy variables for categorical - <b>Locations</b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_titles</th>\n",
       "      <th>companies</th>\n",
       "      <th>locations</th>\n",
       "      <th>reviews</th>\n",
       "      <th>salaries</th>\n",
       "      <th>ratings</th>\n",
       "      <th>summaries</th>\n",
       "      <th>fortune500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vice president  data integration manager  firm...</td>\n",
       "      <td>jp morgan chase</td>\n",
       "      <td>NJ</td>\n",
       "      <td>14863.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>44.4</td>\n",
       "      <td>the organizations objectives are to explain th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jr research analyst</td>\n",
       "      <td>wells fargo</td>\n",
       "      <td>CA</td>\n",
       "      <td>19547.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>43.8</td>\n",
       "      <td>identifies potential data sources for statisti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data specialist solutions</td>\n",
       "      <td>facebook</td>\n",
       "      <td>US</td>\n",
       "      <td>203.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>52.2</td>\n",
       "      <td>effectively plan and collaborate with team mem...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>market data analyst manager vice president</td>\n",
       "      <td>state street</td>\n",
       "      <td>US</td>\n",
       "      <td>1356.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>o automate ec risk data enrichment and data go...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>product data analyst  personalization</td>\n",
       "      <td>hulu</td>\n",
       "      <td>US</td>\n",
       "      <td>28.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>44.4</td>\n",
       "      <td>passion for turning data into insights and hel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          job_titles        companies  \\\n",
       "0  vice president  data integration manager  firm...  jp morgan chase   \n",
       "1                                jr research analyst      wells fargo   \n",
       "2                          data specialist solutions         facebook   \n",
       "3         market data analyst manager vice president     state street   \n",
       "4              product data analyst  personalization             hulu   \n",
       "\n",
       "  locations  reviews  salaries  ratings  \\\n",
       "0        NJ  14863.0   25000.0     44.4   \n",
       "1        CA  19547.0   25000.0     43.8   \n",
       "2        US    203.0   25000.0     52.2   \n",
       "3        US   1356.0   25000.0     42.0   \n",
       "4        US     28.0   25000.0     44.4   \n",
       "\n",
       "                                           summaries  fortune500  \n",
       "0  the organizations objectives are to explain th...           1  \n",
       "1  identifies potential data sources for statisti...           1  \n",
       "2  effectively plan and collaborate with team mem...           1  \n",
       "3  o automate ec risk data enrichment and data go...           0  \n",
       "4  passion for turning data into insights and hel...           0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = RobustScaler()\n",
    "jobs_file.reviews = rs.fit_transform(jobs_file.reviews.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobs_file.ratings = rs.fit_transform(jobs_file.ratings.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobs_nocomp = jobs_file.drop('companies',axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = jobs_nocomp.drop('salaries',axis=1)\n",
    "y = jobs_nocomp['salaries']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=2,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:900px;background:#D9D9D9;border:1px solid black;text-align:left;padding:8px;\">\n",
    "\n",
    "\n",
    "\n",
    "<p>\n",
    "<b>Step 1:</b> Add customized stopwords \n",
    "<br><b>Step 2:</b> Count vectorizer on job_titles, summaries and companies\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unknown locale: UTF-8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-d486ac0bba45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetdefaultlocale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/joannasyeow/anaconda/lib/python2.7/locale.pyc\u001b[0m in \u001b[0;36mgetdefaultlocale\u001b[0;34m(envvars)\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0mlocalename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'C'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parse_localename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocalename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joannasyeow/anaconda/lib/python2.7/locale.pyc\u001b[0m in \u001b[0;36m_parse_localename\u001b[0;34m(localename)\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'C'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'unknown locale: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlocalename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_build_localename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocaletuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unknown locale: UTF-8"
     ]
    }
   ],
   "source": [
    "import locale\n",
    "locale.getdefaultlocale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unknown locale: UTF-8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-83a9ef583191>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#stop = stopwords.words('english')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joannasyeow/anaconda/lib/python2.7/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;31m###########################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joannasyeow/anaconda/lib/python2.7/site-packages/nltk/chunk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChunkParserI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m from nltk.chunk.util import (ChunkScore, accuracy, tagstr2tree, conllstr2tree,\n\u001b[1;32m    159\u001b[0m                              \u001b[0mconlltags2tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree2conlltags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree2conllstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree2conlltags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joannasyeow/anaconda/lib/python2.7/site-packages/nltk/chunk/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParserI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChunkScore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mChunkParserI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParserI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joannasyeow/anaconda/lib/python2.7/site-packages/nltk/chunk/util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstr2tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpython_2_unicode_compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joannasyeow/anaconda/lib/python2.7/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m           \u001b[0;32mimport\u001b[0m \u001b[0mTaggerI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m          \u001b[0;32mimport\u001b[0m \u001b[0mstr2tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple2str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muntag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m from nltk.tag.sequential    import (SequentialBackoffTagger, ContextTagger,\n\u001b[0m\u001b[1;32m     64\u001b[0m                                     \u001b[0mDefaultTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNgramTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnigramTagger\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                                     \u001b[0mBigramTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrigramTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAffixTagger\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joannasyeow/anaconda/lib/python2.7/site-packages/nltk/tag/sequential.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobability\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConditionalFreqDist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNaiveBayesClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpython_2_unicode_compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joannasyeow/anaconda/lib/python2.7/site-packages/nltk/classify/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m                                   ConditionalExponentialClassifier)\n\u001b[1;32m     97\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msenna\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSenna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextcat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextCat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/joannasyeow/anaconda/lib/python2.7/site-packages/nltk/classify/textcat.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# You may have to \"pip install regx\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mregex\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joannasyeow/anaconda/lib/python2.7/site-packages/regex.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[0;31m# We define _pattern_type here after all the support objects have been defined.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m \u001b[0m_pattern_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;31m# We'll define an alias for the 'compile' function so that the repr of a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joannasyeow/anaconda/lib/python2.7/site-packages/regex.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(pattern, flags, kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_locale_sensitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mLOCALE\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;31m# This pattern is, or might be, locale-sensitive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0mpattern_locale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_getlocale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;31m# This pattern is definitely not locale-sensitive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joannasyeow/anaconda/lib/python2.7/locale.pyc\u001b[0m in \u001b[0;36mgetlocale\u001b[0;34m(category)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mLC_ALL\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m';'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlocalename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'category LC_ALL is not supported'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parse_localename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocalename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msetlocale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joannasyeow/anaconda/lib/python2.7/locale.pyc\u001b[0m in \u001b[0;36m_parse_localename\u001b[0;34m(localename)\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'C'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'unknown locale: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlocalename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_build_localename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocaletuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unknown locale: UTF-8"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "#stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(stopwords = stopwords)\n",
    "cvec.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df  = pd.DataFrame(cvec.transform([FEATURE]).todense(),\n",
    "             columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:900px;background:#D9D9D9;border:1px solid black;text-align:left;padding:8px;\">\n",
    "\n",
    "\n",
    "\n",
    "<p>\n",
    "<b>Variance Inflation Factor</b> on numerical data to check for multi-colinearity\n",
    "<br><b>ANOVA Test</b> on binary data\n",
    "<br>Since there is little data avaliable, I will use <b>PCA</b> or <b>LDA</b> to reduce dimensionality\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variance Inflation Factor\n",
    "\n",
    "class VIF(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Variance Inflation Factor\"\"\"\n",
    "    \n",
    "    def __init__(self,threshold=5):\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    \n",
    "    def vif(self,df):\n",
    "        \n",
    "        vif = [variance_inflation_factor(df.iloc[:,:-4].values,i) \\\n",
    "               for i in range(df.iloc[:,:-4].shape[1])]\n",
    "        \n",
    "        vif_df = pd.DataFrame(df.iloc[:,:-4].columns,columns=['Features'])\n",
    "        vif_df['VIF'] = vif    # VIF values in dataframe\n",
    "        \n",
    "        remove_col = list(vif_df[vif_df['VIF']>self.threshold]['Features'])   # Choose only features with VIF < 5\n",
    "        selected_df = df.drop(remove_col,axis=1)\n",
    "        \n",
    "        return selected_df\n",
    "    \n",
    "    def transform(self, df, *args):\n",
    "        \n",
    "        selected_df = self.vif(df)\n",
    "        \n",
    "        return selected_df\n",
    "\n",
    "\n",
    "    def fit(self, df, *args):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Models\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Learning Curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title = 'Learning Curve Random Forest'\n",
    "plot_learning_curve(ESTIMATOR_GS.best_estimator_, title, X, y, ylim=None, cv=None,\\\n",
    "                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
