{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.set(rc={\"figure.figsize\": (15, 8)})\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:900px;background:#F9EECF;border:1px solid black;text-align:left;padding:8px;\">\n",
    "\n",
    "\n",
    "\n",
    "<p>\n",
    "<span style=\"font-size:14pt\"><b>Scraping from indeed.com</span></b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=\"./chromedriver/chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_title(chunk): \n",
    "    \n",
    "    job_titles = []\n",
    "    \n",
    "    try:\n",
    "        title = chunk.find('h2',{'class':'jobtitle'}).get_text()\n",
    "        job_titles.append(title)\n",
    "    except:\n",
    "        job_titles.append(np.nan)\n",
    "        \n",
    "    return job_titles\n",
    "\n",
    "def scrape_company(chunk):\n",
    "    \n",
    "    companies = []\n",
    "    \n",
    "    try:\n",
    "        company = chunk.find('span',{'class':'company'}).get_text()\n",
    "        companies.append(company)\n",
    "    except:\n",
    "        companies.append(np.nan)\n",
    "        \n",
    "    return companies\n",
    "\n",
    "def scrape_location(chunk):\n",
    "    \n",
    "    locations = []\n",
    "    \n",
    "    try:\n",
    "        location = chunk.find('span',{'class':'location'}).get_text()\n",
    "        locations.append(location)\n",
    "    except:\n",
    "        locations.append(np.nan)\n",
    "        \n",
    "    return locations\n",
    "\n",
    "def scrape_review(chunk):\n",
    "    \n",
    "    reviews = []\n",
    "    \n",
    "    try:\n",
    "        review = chunk.find('span',{'class':'slNoUnderline'}).get_text()\n",
    "        review_ = int(review.replace(' reviews','').replace(',',''))\n",
    "        reviews.append(review_)\n",
    "    except:\n",
    "        reviews.append(np.nan)\n",
    "        \n",
    "    return reviews\n",
    "\n",
    "def scrape_rating(chunk):\n",
    "    \n",
    "    ratings = []\n",
    "    \n",
    "    try:\n",
    "        rating = chunk.find('span',{'class':'rating'})\n",
    "        ratings.append(float(rating.get('style').replace('width:','').replace('px','')))\n",
    "        \n",
    "    except:\n",
    "        ratings.append(np.nan)\n",
    "        \n",
    "    return ratings\n",
    "\n",
    "def scrape_salary(chunk):\n",
    "    \n",
    "    salaries = []\n",
    "    \n",
    "    #get_extra = soup.findAll(\"div\", {\"data-tn-component\":\"organicJob\"})\n",
    "\n",
    "    #for chunk in get_extra:\n",
    "\n",
    "    try:\n",
    "        if '$' in chunk.find('span',{'class':'no-wrap'}).get_text().encode('ascii','ignore'):\n",
    "            salary_range = chunk.find('span',{'class':'no-wrap'}).get_text().encode('ascii','ignore')\n",
    "            #salary = re.findall(r'\\d+',salary_range.replace(',',''))\n",
    "\n",
    "            #if 'hour' in salary_range:\n",
    "            #    salary = salary * 2080\n",
    "            #elif 'day' in salary_range:\n",
    "            #    salary = salary * 260\n",
    "            #elif 'week' in salary_range:\n",
    "            #    salary = salary * 52\n",
    "            salaries.append(salary_range)\n",
    "\n",
    "            #salaries.append(np.mean([float(s) for s in salary]))\n",
    "    except:\n",
    "        salaries.append(np.nan)\n",
    "        \n",
    "    return salaries\n",
    "            \n",
    "def scrape_jobtype(chunk):\n",
    "    \n",
    "    job_types = []\n",
    "    \n",
    "    try:\n",
    "        if '$' not in chunk.find('span',{'class':'no-wrap'}).get_text().encode('ascii','ignore'):\n",
    "            job_types.append(chunk.find('span',{'class':'no-wrap'}).get_text().encode('ascii','ignore'))\n",
    "        else:\n",
    "            job_types.append(np.nan)\n",
    "                        \n",
    "    except:\n",
    "        job_types.append(np.nan)\n",
    "        \n",
    "    return job_types\n",
    "            \n",
    "def scrape_summary(chunk):\n",
    "    \n",
    "    summaries = []\n",
    "\n",
    "    try:\n",
    "        summaries.append(chunk.find(\"span\", {\"class\":\"summary\"}).get_text())\n",
    "    except:\n",
    "        summaries.append(np.nan)\n",
    "            \n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......... Number of jobs scraped 10 .........\n",
      "......... Number of jobs scraped 20 .........\n",
      "......... Number of jobs scraped 30 .........\n",
      "......... Number of jobs scraped 40 .........\n",
      "......... Number of jobs scraped 50 .........\n",
      "......... Number of jobs scraped 60 .........\n",
      "......... Number of jobs scraped 70 .........\n",
      "......... Number of jobs scraped 80 .........\n",
      "......... Number of jobs scraped 90 .........\n",
      "......... Number of jobs scraped 100 .........\n",
      "......... Number of jobs scraped 110 .........\n",
      "......... Number of jobs scraped 120 .........\n",
      "......... Number of jobs scraped 130 .........\n",
      "......... Number of jobs scraped 140 .........\n",
      "......... Number of jobs scraped 150 .........\n",
      "......... Number of jobs scraped 160 .........\n",
      "......... Number of jobs scraped 170 .........\n",
      "......... Number of jobs scraped 180 .........\n",
      "......... Number of jobs scraped 190 .........\n",
      "......... Number of jobs scraped 200 .........\n",
      "......... Number of jobs scraped 210 .........\n",
      "......... Number of jobs scraped 220 .........\n",
      "......... Number of jobs scraped 230 .........\n",
      "......... Number of jobs scraped 240 .........\n",
      "......... Number of jobs scraped 250 .........\n",
      "......... Number of jobs scraped 260 .........\n",
      "......... Number of jobs scraped 270 .........\n",
      "......... Number of jobs scraped 280 .........\n",
      "......... Number of jobs scraped 290 .........\n",
      "......... Number of jobs scraped 300 .........\n",
      "......... Number of jobs scraped 310 .........\n",
      "......... Number of jobs scraped 320 .........\n",
      "......... Number of jobs scraped 330 .........\n",
      "......... Number of jobs scraped 340 .........\n",
      "......... Number of jobs scraped 350 .........\n",
      "......... Number of jobs scraped 360 .........\n",
      "......... Number of jobs scraped 370 .........\n",
      "......... Number of jobs scraped 380 .........\n",
      "......... Number of jobs scraped 390 .........\n",
      "......... Number of jobs scraped 400 .........\n",
      "......... Number of jobs scraped 410 .........\n",
      "......... Number of jobs scraped 420 .........\n",
      "......... Number of jobs scraped 430 .........\n",
      "......... Number of jobs scraped 440 .........\n",
      "......... Number of jobs scraped 450 .........\n",
      "......... Number of jobs scraped 460 .........\n",
      "......... Number of jobs scraped 470 .........\n",
      "......... Number of jobs scraped 480 .........\n",
      "......... Number of jobs scraped 490 .........\n",
      "......... Number of jobs scraped 500 .........\n",
      "......... Number of jobs scraped 510 .........\n",
      "......... Number of jobs scraped 520 .........\n",
      "......... Number of jobs scraped 530 .........\n",
      "......... Number of jobs scraped 540 .........\n",
      "......... Number of jobs scraped 550 .........\n",
      "......... Number of jobs scraped 560 .........\n",
      "......... Number of jobs scraped 570 .........\n",
      "......... Number of jobs scraped 580 .........\n",
      "......... Number of jobs scraped 590 .........\n",
      "......... Number of jobs scraped 600 .........\n",
      "......... Number of jobs scraped 610 .........\n",
      "......... Number of jobs scraped 620 .........\n",
      "......... Number of jobs scraped 630 .........\n",
      "......... Number of jobs scraped 640 .........\n",
      "......... Number of jobs scraped 650 .........\n",
      "......... Number of jobs scraped 660 .........\n",
      "......... Number of jobs scraped 670 .........\n",
      "......... Number of jobs scraped 680 .........\n",
      "......... Number of jobs scraped 690 .........\n",
      "......... Number of jobs scraped 700 .........\n",
      "......... Number of jobs scraped 710 .........\n",
      "......... Number of jobs scraped 720 .........\n",
      "......... Number of jobs scraped 730 .........\n",
      "......... Number of jobs scraped 740 .........\n",
      "......... Number of jobs scraped 750 .........\n",
      "......... Number of jobs scraped 760 .........\n",
      "......... Number of jobs scraped 770 .........\n",
      "......... Number of jobs scraped 780 .........\n",
      "......... Number of jobs scraped 790 .........\n",
      "......... Number of jobs scraped 800 .........\n",
      "......... Number of jobs scraped 810 .........\n",
      "......... Number of jobs scraped 820 .........\n",
      "......... Number of jobs scraped 830 .........\n",
      "......... Number of jobs scraped 840 .........\n",
      "......... Number of jobs scraped 850 .........\n",
      "......... Number of jobs scraped 860 .........\n",
      "......... Number of jobs scraped 870 .........\n",
      "......... Number of jobs scraped 880 .........\n",
      "......... Number of jobs scraped 890 .........\n",
      "......... Number of jobs scraped 900 .........\n",
      "......... Number of jobs scraped 910 .........\n",
      "......... Number of jobs scraped 920 .........\n",
      "......... Number of jobs scraped 930 .........\n",
      "......... Number of jobs scraped 940 .........\n"
     ]
    },
    {
     "ename": "SSLError",
     "evalue": "(\"bad handshake: SysCallError(-1, 'Unexpected EOF')\",)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8d5c303085e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://www.indeed.com/jobs?q=data+%24'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'%2C000&start='\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"data-tn-component\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"organicJob\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joannasyeow/anaconda/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joannasyeow/anaconda/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joannasyeow/anaconda/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    516\u001b[0m         }\n\u001b[1;32m    517\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joannasyeow/anaconda/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joannasyeow/anaconda/lib/python2.7/site-packages/requests/adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_SSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_HTTPError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_SSLError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReadTimeoutError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mReadTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSSLError\u001b[0m: (\"bad handshake: SysCallError(-1, 'Unexpected EOF')\",)"
     ]
    }
   ],
   "source": [
    "pages = [range(0,1500,10)]\n",
    "salary_amt = ['25','35','50','65','80']\n",
    "\n",
    "for amt in salary_amt:\n",
    "    \n",
    "    for ind, batch in enumerate(pages):\n",
    "        \n",
    "        job_titles_ = []\n",
    "        companies_ = []\n",
    "        locations_ = []\n",
    "        reviews_ = []\n",
    "        salaries_ = []\n",
    "        ratings_ = []\n",
    "        summaries_ = []\n",
    "        \n",
    "        for i in batch:\n",
    "    \n",
    "            url = 'https://www.indeed.com/jobs?q=data+%24'+str(amt)+'%2C000&start='+str(i)\n",
    "            response = requests.get(url)\n",
    "            html = response.text\n",
    "            soup = BeautifulSoup(html, 'lxml').find_all('div', {\"data-tn-component\":\"organicJob\"})\n",
    "\n",
    "            for chunk in soup:\n",
    "\n",
    "                job_titles = job_titles_.extend(scrape_title(chunk))\n",
    "                companies = companies_.extend(scrape_company(chunk))\n",
    "                locations = locations_.extend(scrape_location(chunk))\n",
    "                reviews = reviews_.extend(scrape_review(chunk))\n",
    "                salaries = salaries_.extend(scrape_salary(chunk))\n",
    "                ratings = ratings_.extend(scrape_rating(chunk))\n",
    "                summaries = summaries_.extend(scrape_summary(chunk))\n",
    "\n",
    "            print '......... Number of jobs scraped ' + str(i + 10)+ ' .........'\n",
    "\n",
    "            time.sleep(5)\n",
    "\n",
    "        results = pd.DataFrame()\n",
    "\n",
    "        results['job_titles'] = job_titles_\n",
    "        results['companies'] = companies_\n",
    "        results['locations'] = locations_\n",
    "        results['reviews'] = reviews_\n",
    "        results['salaries'] = salaries_\n",
    "        results['ratings'] = ratings_\n",
    "        results['summaries'] = summaries_\n",
    "        \n",
    "        results.to_pickle('results_'+str(amt)+'000_'+str(ind)+'.pickle')\n",
    "        \n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
